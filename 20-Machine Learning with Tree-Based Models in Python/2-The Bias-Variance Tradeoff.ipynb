{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of supervised learning is to find a f(head) that best approximates f.(with logistic regression, decision tree, neural network...) When training fhat the noise should be discarded as much as possible. At the end fhat should achieve a low predictive error on unseen datasets. \n",
    "\n",
    "There are 2 difficulties while approximating f.\n",
    "Overfitting: when fhat fits the noise in the trainin set.\n",
    "Underfitting: When fhat is not flexible enough to approximate f.\n",
    "\n",
    "With overfitting, model's predictive power on unseen datasets is pretty low. (low training set error - high test set error.)\n",
    "\n",
    "With underfitting the training set error is roughly equal to the test set error and both relatively high.\n",
    "\n",
    "The generalization error of a model tells you how much it generalizes on unseen data. -> Bias, variance, irreducible error\n",
    "\n",
    "irreducible error: error contribution of noise\n",
    "\n",
    "bias: tells how much fhat and f are different. High bias models lead to underfitting.\n",
    "\n",
    "variance: tells you how much fhat is inconsistent over different training sets.\n",
    "fhat of a high variance model follows training data points too close and it leads to overfitting.\n",
    "\n",
    "The complexity of a model sets its flexibility to approximate the true function of f. Increased tree depth -> increased complexity of a disicion tree.\n",
    "\n",
    "Increased model complexity the bias decreases and variance increases. Finding the model complexity that achieves the lowest generalization error. So find a balance between bias and variance. -> bias-variance trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnose bias and variance problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model fhat can then be fit to the training set and its error can be evaluated on the test set. The generalization error of fhat is roughly approximated by fhat's error on the test set.To obtain a reliable estimate of fhat's performance, you should use a technique called cross-validation or CV. \n",
    "Once fhat's cross-validation-error is computed, it can be checked if it is greater than fhat's training set error. If it is greater, fhat is said to suffer from high variance. In such case, fhat has overfit the training set. To remedy this try decreasing fhat's complexity.\n",
    "\n",
    "On the other hand, fhat is said to suffer from high bias if its cross-validation-error is roughly equal to the training error but much greater than the desired error. In such case fhat underfits the training set. To remedy this try increasing the model's complexity or gather more relevant features for the problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "cars = pd.read_csv(\"mpg.csv\")\n",
    "df_origin = pd.get_dummies(cars)\n",
    "X = df_origin.drop(\"mpg\", axis=1)\n",
    "y = df_origin[\"mpg\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "dt = DecisionTreeRegressor(min_samples_leaf = 0.26, max_depth=4, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the 10-fold CV error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross_val_score has only the option of evaluating the negative MSEs, its output should be multiplied by negative one to obtain the MSEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV RMSE: 5.14\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "MSE_CV_scores = - cross_val_score(dt, X_train, y_train, cv=10, scoring=\"neg_mean_squared_error\",\n",
    "                                 n_jobs=-1)\n",
    "RMSE_CV = (MSE_CV_scores.mean()) ** 0.5\n",
    "print('CV RMSE: {:.2f}'.format(RMSE_CV))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " CV is a great technique to get an estimate of a model's performance without affecting the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 5.15\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_train = dt.predict(X_train)\n",
    "RMSE_train = (mean_squared_error(y_train, y_pred_train))**0.5\n",
    "print('Train RMSE: {:.2f}'.format(RMSE_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training error is roughly equal to the 10-folds CV error you obtained in the previous exercise. dt suffers from high bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CARTs are sensitive to small variations. The also suffer from high variance and may overfit it they trained without constrains. A solution that takes advantage of the flexibility of CARTs while reducing their tendency to memorize noise is ensemble learning.\n",
    "It is training different models on the same dataset, let each model make its predictions, aggregate predictions of individual models, and make final prediction that is more robust and less prone to errors than each individual in the model. Best result is recieved when models are skillful in different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.936\n",
      "K Nearest Neighbor: 0.930\n",
      "Classification Tree: 0.930\n",
      "Voting Classifier: 0.959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cancer = pd.read_csv(\"cancer.csv\")\n",
    "X = cancer.drop([\"id\", \"Unnamed: 32\", \"diagnosis\"], axis=1)\n",
    "y = cancer[\"diagnosis\"]\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y = pd.Series(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
    "                                                   random_state=1)\n",
    "lr = LogisticRegression(random_state=1)\n",
    "knn = KNN()\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "classifiers = [(\"Logistic Regression\", lr),\n",
    "               (\"K Nearest Neighbor\", knn),\n",
    "               (\"Classification Tree\", dt)]\n",
    "\n",
    "for clf_name, clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"{:s}: {:.3f}\".format(clf_name, accuracy_score(y_test, y_pred)))\n",
    "    \n",
    "vc = VotingClassifier(estimators=classifiers)\n",
    "vc.fit(X_train, y_train)\n",
    "y_pred = vc.predict(X_test)\n",
    "print(\"Voting Classifier: {:.3f}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is higher than that achieved by any of the individual models in the ensemble.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=1)\n",
    "knn = KNN(n_neighbors=27)\n",
    "dt = DecisionTreeClassifier(min_samples_leaf=0.13, random_state=1)\n",
    "classifiers = [('Logistic Regression', lr),\n",
    "               ('K Nearest Neighbours', knn),\n",
    "               ('Classification Tree', dt)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate individual classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression : 0.759\n",
      "K Nearest Neighbours : 0.701\n",
      "Classification Tree : 0.730\n"
     ]
    }
   ],
   "source": [
    "liver = pd.read_csv(\"indian_liver_patient_preprocessed.csv\", index_col=0)\n",
    "X = liver.drop(\"Liver_disease\", axis=1)\n",
    "y = liver[\"Liver_disease\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) \n",
    "\n",
    "for clf_name, clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('{:s} : {:.3f}'.format(clf_name, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better performance with a Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier: 0.770\n"
     ]
    }
   ],
   "source": [
    "vc = VotingClassifier(estimators=classifiers)\n",
    "vc.fit(X_train, y_train)\n",
    "y_pred = vc.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Voting Classifier: {:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
