{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning a CART's Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain a better perforance, the hyperparameters of a machine learning model should be tuned.\n",
    "\n",
    "Optimal model yields an optimal score. Score in sklearn defaults to accuracy (classification) and R2 (regression). A model's generalization performance is evaluated using cross-validation.\n",
    "\n",
    "Grid_search is one of the hyperparameter tuning methods. Suffers from the curse of dimensionality, the bigger the grid the longer it takes to find the solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'presort': 'deprecated', 'random_state': 1, 'splitter': 'best'}\n",
      "Best hyperparameters:  {'max_depth': 4, 'max_features': 0.2, 'min_samples_leaf': 0.06}\n",
      "Best CV accuracy 0.935\n",
      "Test set accuracy of best model: 0.906\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "cancer = pd.read_csv(\"cancer.csv\")\n",
    "X = cancer.drop([\"id\", \"Unnamed: 32\", \"diagnosis\"], axis=1)\n",
    "y = cancer[\"diagnosis\"]\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y = pd.Series(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
    "                                                   stratify=y, random_state=1)\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "print(dt.get_params())\n",
    "\n",
    "params_dt = {\"max_depth\":[3, 4, 5, 6],\n",
    "            \"min_samples_leaf\":[0.04, 0.06, 0.08],\n",
    "            \"max_features\":[0.2, 0.4, 0.6, 0.8]\n",
    "            }\n",
    "\n",
    "grid_dt = GridSearchCV(estimator=dt, param_grid=params_dt, scoring=\"accuracy\",cv=10,n_jobs=-1)\n",
    "grid_dt.fit(X_train, y_train)\n",
    "best_hyperparams = grid_dt.best_params_\n",
    "print(\"Best hyperparameters: \", best_hyperparams)\n",
    "best_CV_score = grid_dt.best_score_\n",
    "print(\"Best CV accuracy {:.3f}\".format(best_CV_score))\n",
    "\n",
    "best_model = grid_dt.best_estimator_\n",
    "test_acc = best_model.score(X_test, y_test)\n",
    "print(\"Test set accuracy of best model: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the tree's hyperparameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dt = {\n",
    "    \"max_depth\": [2,3,4],\n",
    "    \"min_samples_leaf\": [0.12, 0.14, 0.16, 0.18]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for the optimal tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "liver = pd.read_csv(\"indian_liver_patient_preprocessed.csv\", index_col=0)\n",
    "X = liver.drop(\"Liver_disease\", axis=1)\n",
    "y = liver[\"Liver_disease\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) \n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "grid_dt = GridSearchCV(estimator=dt, param_grid=params_dt, scoring=\"roc_auc\",cv=5,n_jobs=-1, refit=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the optimal tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set ROC AUC score of grid_dt: 0.731\n",
      "Test set ROC AUC score of dt: 0.598\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "grid_dt.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_dt.best_estimator_\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:,1]\n",
    "test_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print('Test set ROC AUC score of grid_dt: {:.3f}'.format(test_roc_auc))\n",
    "\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_proba = dt.predict_proba(X_test)[:,1]\n",
    "test_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print('Test set ROC AUC score of dt: {:.3f}'.format(test_roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An untuned classification-tree would achieve a ROC AUC score of 0.54"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning a RF's Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computational expensive and sometimes leads to very little improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'mse', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 1, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(random_state=1)\n",
    "print(rf.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the hyperparameter grid of RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_rf = {\n",
    "    \"n_estimators\":[100, 350, 500],\n",
    "    \"max_features\":[\"log2\", 'auto', 'sqrt' ],\n",
    "    \"min_samples_leaf\":[2, 10, 30]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for the optimal forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state=1)\n",
    "grid_rf = GridSearchCV(estimator=rf,\n",
    "                       param_grid=params_rf,\n",
    "                       scoring=\"neg_mean_squared_error\",\n",
    "                       cv=3,\n",
    "                       verbose=1,\n",
    "                       n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the optimal forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done  81 out of  81 | elapsed:   17.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE of best model: 61.598\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "bike = pd.read_csv(\"bike.csv\")\n",
    "X = bike.drop(\"cnt\", axis=1)\n",
    "y = bike[\"cnt\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=6)\n",
    "grid_rf.fit(X_train, y_train)\n",
    "best_model = grid_rf.best_estimator_\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Compute rmse_test\n",
    "rmse_test = MSE(y_test, y_pred) ** 0.5\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test RMSE of best model: {:.3f}'.format(rmse_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
