{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"pyspark-shell\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to data cleaning with Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing raw data for use in data processing pipelines. Spark is scalable so lets you scale your data processing capacity.\n",
    "\n",
    "The primary limit to Spark's abilities is the **level of RAM** in the Spark cluster.\n",
    "\n",
    "A schema defines and validates the number and types of columns for a given DataFrame. Defining a schema also improves read performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "people_schema = StructType([\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"age\", IntegerType(), False),\n",
    "    StructField(\"city\", StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immutability and lazy processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark DataFrames are defined once and are not modifiable after initialization.\n",
    "\n",
    "Lazy processing makes Spark to perform the most efficient set of operations to get the desired result. Spark not performing any transformations until an action is requested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using lazy processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Actual elapsed time (Minutes)|airport|\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|       01/01/2017|         0005|                          537|    hnl|\n",
      "|       01/01/2017|         0007|                          498|    ogg|\n",
      "|       01/01/2017|         0037|                          241|    sfo|\n",
      "|       01/01/2017|         0043|                          134|    dtw|\n",
      "|       01/01/2017|         0051|                           88|    stl|\n",
      "|       01/01/2017|         0060|                          149|    mia|\n",
      "|       01/01/2017|         0071|                          203|    lax|\n",
      "|       01/01/2017|         0074|                           76|    mem|\n",
      "|       01/01/2017|         0081|                          123|    den|\n",
      "|       01/01/2017|         0089|                          161|    slc|\n",
      "|       01/01/2017|         0096|                           84|    stl|\n",
      "|       01/01/2017|         0103|                          216|    sjc|\n",
      "|       01/01/2017|         0119|                          514|    ogg|\n",
      "|       01/01/2017|         0123|                          529|    hnl|\n",
      "|       01/01/2017|         0126|                          171|    lga|\n",
      "|       01/01/2017|         0132|                          188|    ewr|\n",
      "|       01/01/2017|         0140|                          231|    sjc|\n",
      "|       01/01/2017|         0174|                          145|    rdu|\n",
      "|       01/01/2017|         0176|                          184|    bos|\n",
      "|       01/01/2017|         0190|                           76|    sat|\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "aa_dfw_df = spark.read.csv(\"AA_DFW_2017_Departures_Short.csv\", header=True)\n",
    "aa_dfw_df = aa_dfw_df.withColumn(\"airport\", F.lower(aa_dfw_df[\"Destination Airport\"]))\n",
    "aa_dfw_df = aa_dfw_df.drop(aa_dfw_df[\"Destination Airport\"])\n",
    "aa_dfw_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some common issues with CSV files are; the schema is not defined, there are no data types included, nor columns names (beyond a header row). In addition to there, Spark has some specific problems processing CSV data. CSV files are quire slow to import and parse. \n",
    "\n",
    "Parquet is a compressed columnar data format developed for use in any Hadoop based system. The Parquet format is structured with data accessible in chunks, allowing efficient read and write operations without processing the entire file. It provides significant performance improvement and they automatically include schema information and handle data encoding.\n",
    "\n",
    "Parquet files are binary file format and can only be used with the proper tools. \n",
    "\n",
    "To write parquet file use: df.write.parquet(\"filename.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving a DataFrame in Parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 Count: 139359\n",
      "df2 Count: 140605\n",
      "279964\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.csv(\"AA_DFW_2017_Departures_Short.csv\")\n",
    "df2 = spark.read.csv(\"AA_DFW_2016_Departures_Short.csv\")\n",
    "\n",
    "print(\"df1 Count: %d\"%df1.count())\n",
    "print(\"df2 Count: %d\"%df2.count())\n",
    "\n",
    "df3 = df1.union(df2)\n",
    "df3 = df3.toPandas()\n",
    "df3.to_parquet(\"AA_DFW_ALL.parquet\", engine='pyarrow', compression='gzip', index=False)\n",
    "# df3.write.parquet(\"AA_DFW_ALL.parquet\", mode=\"overwrite\")\n",
    "print(spark.read.parquet('AA_DFW_ALL.parquet').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL and Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average flight time is: 151\n"
     ]
    }
   ],
   "source": [
    "flights_df = spark.read.parquet('AA_DFW_ALL.parquet')\n",
    "flights_df.createOrReplaceTempView(\"flights\")\n",
    "\n",
    "avg_duration = spark.sql(\"SELECT avg(_c3) FROM flights\").collect()[0]\n",
    "print('The average flight time is: %d' % avg_duration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
