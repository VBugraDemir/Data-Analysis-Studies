{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"pyspark-shell\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance can be improved by using caching. Caching in Spark refers to storing the results of a DataFrame in memory or on disk of the processing nodes in a cluster. Improves speed on later transformations/actions since data no longer needs to be retrieved from the original data source. \n",
    "\n",
    "Very large data sets may not fit in memory. Cacing is incredibly useful but only if you plan to use the DataFrame again. For a single task it is not worth it. If normal caching doesn't seem to work, try creating intermediate Parquet representations. Cache is a Spark transformation (lazy). So nothing is actually cached until an action is called. Use .unpersist() to remove the object from the cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting 139358 rows took 9.225997 seconds\n",
      "Counting 139358 rows took 2.340655 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "departures_df = spark.read.csv(\"AA_DFW_2017_Departures_Short.csv\", header= True)\n",
    "start_time = time.time()\n",
    "departures_df = departures_df.distinct().cache()\n",
    "\n",
    "print(\"Counting %d rows took %f seconds\" % (departures_df.count(), time.time() - start_time))\n",
    "start_time = time.time()\n",
    "print(\"Counting %d rows took %f seconds\" % (departures_df.count(), time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing a DataFrame from cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is departures_df cached?: True\n",
      "Removing departures_df from cache\n",
      "Is departures_df cached?: False\n"
     ]
    }
   ],
   "source": [
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)\n",
    "print(\"Removing departures_df from cache\")\n",
    "\n",
    "departures_df.unpersist()\n",
    "\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve import performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark clusters are made of two types of processes. Driver process and worker processes. The driver handles task assignments and consolidation of the data results from the workers. The workers handle the actual transformation/action tasks of a Spark job. Once assigned tasks, they operate independently and reports results back to the driver.\n",
    "\n",
    "The more import objects the better the cluster can divvy up the job. One large file will perform worse than many smaller ones. Dependinf on the configuration of you cluster, you may not be able to process larger files, but could easily handle the same amount of data split between smaller files.\n",
    "\n",
    "You can define a single import statement, even if there are multiple files by using wildcard symbol.\n",
    "\n",
    "Spark performs better if objects are of similar size.\n",
    "\n",
    "Well dedined schema will imporve import performance by avoiding reading the data multiple times.\n",
    "\n",
    "If a DataFrame will be used often, a simple method is to read in the single filen then write it back out as parquet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File import performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in full DataFrame:\t583723\n",
      "Time to run: 0.614811\n"
     ]
    }
   ],
   "source": [
    "full_df = spark.read.csv(\"departures_full.csv\")\n",
    "# split_df = spark.read.csv(\"C:\\\\Users\\\\Buğra\\\\Downloads\\\\AA_DFW_*_Departures_Short.csv.gz\")\n",
    "# Couldn't figure out to read multiple files at the same time. Somehow it tooks too long to read multiple files.\n",
    "\n",
    "start_time_a = time.time()\n",
    "print(\"Total rows in full DataFrame:\\t%d\" % full_df.count())\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_a))\n",
    "\n",
    "# start_time_b = time.time()\n",
    "# print(\"Total rows in split DataFrame:\\t%d\" % split_df.count())\n",
    "# print(\"Time to run: %f\" % (time.time() - start_time_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark contains many configuration settings and these can be modified to match needs. The configurations are available in the configuration files, via the Spark web interface and via the run-time code. Use spark.conf.get(< configuration name >) to read settings. spark.conf.set(< configuration name >) to set. \n",
    "\n",
    "Spark deployment options: Single node clusters, Standalone clusters with dedicated machines ad the driver and workers or Managed clusters that components are handled by a third party cluster (YARN, Mesos, Kubernetes).\n",
    "\n",
    "Driver handles task assignment to the various nodes/processes in the cluster. Result consolidation.\n",
    "\n",
    "Driver node should have double the memory of the worker. Fast local storage is helpful. \n",
    "\n",
    "A spark worker handles running tasks assigned by the driver and communicates those results back to the driver. More worker nodes are better than more larger nodes. You need to test configs to find balance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Spark configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyspark-shell\n",
      "Driver TCP port: 51673\n",
      "Number of partitions: 200\n"
     ]
    }
   ],
   "source": [
    "app_name = spark.conf.get('spark.app.name')\n",
    "driver_tcp_port = spark.conf.get('spark.driver.port')\n",
    "num_partitions = spark.conf.get('spark.sql.shuffle.partitions')\n",
    "print(\"Name: %s\"%app_name)\n",
    "print(\"Driver TCP port: %s\" % driver_tcp_port)\n",
    "print(\"Number of partitions: %s\" % num_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Spark configurations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition count before change: 200\n",
      "Partition count after change: 500\n"
     ]
    }
   ],
   "source": [
    "before = departures_df.rdd.getNumPartitions()\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 500)\n",
    "\n",
    "departures_df = spark.read.csv(\"AA_DFW_2017_Departures_Short.csv\", header= True).distinct()\n",
    "\n",
    "print(\"Partition count before change: %d\" % before)\n",
    "print(\"Partition count after change: %d\" % departures_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improving performance of Spark tasks in general. To understand performance implications of Spark, it should be understood what it's doing under the hood. For that use explain() funtion on a DataFrame.\n",
    "\n",
    "Shuffling refers to moving data around to various workers to complete a task. It hides complexity from the user. Can be slow to complete and lowers overall throughput. It is necessary but try to minimize it.\n",
    "\n",
    "Repartitioning is quite costly. Use coalesce function instead. Join can often cause shuffle operations.\n",
    "\n",
    "Broadcasting provides a copy of an object to each worker. This decrease need for communication between nodes. This limits data shuffles and it's more likely a node will fulfill tasks independently. Broadcasting can speed up .join() operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [Destination Airport#364], [IATA#343], Inner, BuildRight, false\n",
      ":- *(2) Filter isnotnull(Destination Airport#364)\n",
      ":  +- FileScan csv [Date (MM/DD/YYYY)#362,Flight Number#363,Destination Airport#364,Actual elapsed time (Minutes)#365] Batched: false, DataFilters: [isnotnull(Destination Airport#364)], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/Buğra/Datacamp-jupyter_notebook/PySpark/Cleaning Data with PySpa..., PartitionFilters: [], PushedFilters: [IsNotNull(Destination Airport)], ReadSchema: struct<Date (MM/DD/YYYY):string,Flight Number:string,Destination Airport:string,Actual elapsed ti...\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [id=#237]\n",
      "   +- *(1) Project [_c1#311 AS AIRPORTNAME#340, _c4#314 AS IATA#343]\n",
      "      +- *(1) Filter isnotnull(_c4#314)\n",
      "         +- FileScan csv [_c1#311,_c4#314] Batched: false, DataFilters: [isnotnull(_c4#314)], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/Buğra/Datacamp-jupyter_notebook/PySpark/Cleaning Data with PySpa..., PartitionFilters: [], PushedFilters: [IsNotNull(_c4)], ReadSchema: struct<_c1:string,_c4:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports_df = spark.read.csv(\"airport.csv\").select(\"_c1\", \"_c4\")\n",
    "airports_df = airports_df.withColumnRenamed(\"_c1\", \"AIRPORTNAME\")\n",
    "airports_df = airports_df.withColumnRenamed(\"_c4\", \"IATA\")\n",
    "\n",
    "flights_df = spark.read.csv(\"AA_DFW_2017_Departures_Short.csv\", header=True)\n",
    "\n",
    "normal_df = flights_df.join(airports_df, flights_df[\"Destination Airport\"] == airports_df[\"IATA\"])\n",
    "normal_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using broadcasting on Spark joins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [Destination Airport#364], [IATA#343], Inner, BuildRight, false\n",
      ":- *(2) Filter isnotnull(Destination Airport#364)\n",
      ":  +- FileScan csv [Date (MM/DD/YYYY)#362,Flight Number#363,Destination Airport#364,Actual elapsed time (Minutes)#365] Batched: false, DataFilters: [isnotnull(Destination Airport#364)], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/Buğra/Datacamp-jupyter_notebook/PySpark/Cleaning Data with PySpa..., PartitionFilters: [], PushedFilters: [IsNotNull(Destination Airport)], ReadSchema: struct<Date (MM/DD/YYYY):string,Flight Number:string,Destination Airport:string,Actual elapsed ti...\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [id=#269]\n",
      "   +- *(1) Project [_c1#311 AS AIRPORTNAME#340, _c4#314 AS IATA#343]\n",
      "      +- *(1) Filter isnotnull(_c4#314)\n",
      "         +- FileScan csv [_c1#311,_c4#314] Batched: false, DataFilters: [isnotnull(_c4#314)], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/Buğra/Datacamp-jupyter_notebook/PySpark/Cleaning Data with PySpa..., PartitionFilters: [], PushedFilters: [IsNotNull(_c4)], ReadSchema: struct<_c1:string,_c4:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "broadcast_df = flights_df.join(broadcast(airports_df), flights_df[\"Destination Airport\"] == airports_df[\"IATA\"])\n",
    "\n",
    "broadcast_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing broadcast vs normal joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal count:\t\t139358\tduration: 0.577108\n",
      "Broadcast count:\t139358\tduration: 0.398370\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "normal_count = normal_df.count()\n",
    "normal_duration = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "broadcast_count = broadcast_df.count()\n",
    "broadcast_duration = time.time() - start_time\n",
    "\n",
    "print(\"Normal count:\\t\\t%d\\tduration: %f\" % (normal_count, normal_duration))\n",
    "print(\"Broadcast count:\\t%d\\tduration: %f\" % (broadcast_count, broadcast_duration))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
