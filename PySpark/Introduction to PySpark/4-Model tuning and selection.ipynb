{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"pyspark-shell\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "flights = spark.read.csv(\"flights.csv\", header=True)\n",
    "flights.createOrReplaceTempView(\"flights\")\n",
    "\n",
    "planes = spark.read.csv(\"planes.csv\", header=True)\n",
    "planes.createOrReplaceTempView(\"planes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "planes = planes.withColumnRenamed(\"year\", \"plane_year\")\n",
    "model_data = flights.join(planes, on=\"tailnum\", how=\"leftouter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = model_data.withColumn(\"arr_delay\", model_data.arr_delay.cast(\"integer\"))\n",
    "model_data = model_data.withColumn(\"air_time\", model_data.air_time.cast(\"integer\"))\n",
    "model_data = model_data.withColumn(\"month\", model_data.month.cast(\"integer\"))\n",
    "model_data = model_data.withColumn(\"plane_year\", model_data.plane_year.cast(\"integer\"))\n",
    "\n",
    "model_data = model_data.withColumn(\"plane_age\", model_data.year - model_data.plane_year)\n",
    "\n",
    "model_data = model_data.withColumn(\"is_late\", model_data.arr_delay>0)\n",
    "model_data = model_data.withColumn(\"label\", model_data.is_late.cast(\"integer\"))\n",
    "model_data = model_data.filter(\"arr_delay is not NULL AND dep_delay is not NULL AND air_time is not NULL AND plane_year is not NULL\")\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "carr_indexer = StringIndexer(inputCol = \"carrier\", outputCol= \"carrier_index\")\n",
    "carr_encoder = OneHotEncoder(inputCol= \"carrier_index\", outputCol= \"carrier_fact\")\n",
    "\n",
    "dest_indexer = StringIndexer(inputCol = \"dest\", outputCol=\"dest_index\")\n",
    "dest_encoder = OneHotEncoder(inputCol = \"dest_index\", outputCol=\"dest_fact\")\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=[\"month\", \"air_time\", \"carrier_fact\", \"dest_fact\", \"plane_age\"], outputCol=\"features\")\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "flights_pipe = Pipeline(stages = [dest_indexer, dest_encoder, carr_indexer, carr_encoder, vec_assembler])\n",
    "\n",
    "piped_data = flights_pipe.fit(model_data).transform(model_data)\n",
    "training, test = piped_data.randomSplit([0.6, 0.4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model tuning and selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is logistic regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is very similar to a linear regression, but instead of predicting a numeric variable, it predicts the probability (between 0 and 1) of an event. To use this as a classification algorithm, all you have to do is assign a cutoff point to these probabilities. If the predicted probability is above the cutoff point, you classify that observation as a 'yes' (in this case, the flight being late), if it's below, you classify it as a 'no'!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the modeler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works by splitting the training data into a few different partitions. The exact number is up to you, but in this course you'll be using PySpark's default value of three. Once the data is split up, one of the partitions is set aside, and the model is fit to the others. Then the error is measured against the held out partition. This is repeated for each of the partitions, so that every block of data is held out and used as a test set exactly once. Then the error on each of the partitions is averaged. This is called the cross validation error of the model, and is a good estimate of the actual error on the held out data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing you need when doing cross validation for model selection is a way to compare different models. The pyspark.ml.evaluation submodule has classes for evaluating different kinds of models. Your model is a binary classification model, so you'll be using the BinaryClassificationEvaluator from the pyspark.ml.evaluation.\n",
    "\n",
    "This evaluator calculates the area under the ROC. This is a metric that combines the two kinds of errors a binary classifier can make (false positives and false negatives) into a simple number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.evaluation as evals\n",
    "\n",
    "evaluator = evals.BinaryClassificationEvaluator(metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to create a grid of values to search over when looking for the optimal hyperparameters. The submodule pyspark.ml.tuning includes a class called ParamGridBuilder that does just that.\n",
    "\n",
    "You'll need to use the .addGrid() and .build() methods to create a grid that you can use for cross validation. The .addGrid() method takes a model parameter (an attribute of the model Estimator, lr, that you created a few exercises ago) and a list of values that you want to try. The .build() method takes no arguments, it just returns the grid that you'll use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.tuning as tune\n",
    "import numpy as np\n",
    "grid = tune.ParamGridBuilder()\n",
    "grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\n",
    "grid = grid.addGrid(lr.elasticNetParam, [0, 1])\n",
    "grid = grid.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the validator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The submodule pyspark.ml.tuning also has a class called CrossValidator for performing cross validation. This Estimator takes the modeler you want to fit, the grid of hyperparameters you created, and the evaluator you want to use to compare your models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = tune.CrossValidator(estimator = lr,\n",
    "                        estimatorParamMaps=grid,\n",
    "                        evaluator=evaluator\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionModel: uid=LogisticRegression_d046a9b49c7c, numClasses=2, numFeatures=81\n"
     ]
    }
   ],
   "source": [
    "best_lr = lr.fit(training)\n",
    "print(best_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7014920809637379\n"
     ]
    }
   ],
   "source": [
    "test_results = best_lr.transform(test)\n",
    "print(evaluator.evaluate(test_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossValidatorModel_78439ccf4353\n"
     ]
    }
   ],
   "source": [
    "best_lr = cv.fit(training)\n",
    "print(best_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7014920809637379\n"
     ]
    }
   ],
   "source": [
    "test_results = best_lr.transform(test)\n",
    "print(evaluator.evaluate(test_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
