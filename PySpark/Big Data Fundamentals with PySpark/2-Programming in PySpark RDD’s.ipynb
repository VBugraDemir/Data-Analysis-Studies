{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming in PySpark RDDâ€™s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstracting Data with RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resilient Distributed Datasets. Collection of data distributed across the cluster. 3 important properties of RDD:\n",
    "\n",
    "    Resilient: Ability to withstand failures\n",
    "    Distributed: Spanning across multiple machines\n",
    "    Datasets: Collection of partitioned data like Arrays, Tables, Tuples.\n",
    "    \n",
    "Common way to create RDDs is to lead data from external datasets such as files stored in HDFS or objects in Amazon S3 bucket or lines in a text file stored locally and pass it to SparkContext's textFile method. Also RDDs can also be created from existing RDDs which we will see in the next video.\n",
    "\n",
    "A partition is a logical division of a large distributed data set. Minimum number of partitions can be created for an RDD. The number of partitons in an RDD can always be found by using the getNumPartitions method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDDs from Parallelized collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"pyspark-shell\")\n",
    "\n",
    "RDD = sc.parallelize([\"Spark\", \"is\", \"a\", \"framework\", \"for\", \"Big Data processing\"])\n",
    "\n",
    "print(type(RDD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDDs from External Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file type of fileRDD is <class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "fileRDD = sc.textFile(\"README.md\")\n",
    "\n",
    "print(\"The file type of fileRDD is\", type(fileRDD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitions in your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions in fileRDD is 1\n",
      "Number of partitions in fileRDD_part is 9\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of partitions in fileRDD is\", fileRDD.getNumPartitions())\n",
    "\n",
    "fileRDD_part = sc.textFile(\"README.md\", minPartitions = 5)\n",
    "\n",
    "print(\"Number of partitions in fileRDD_part is\", fileRDD_part.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RDD Transformations and Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs in PySpark supports two different types of operations. Transformations and actions. Transformations create new RDDs and actions perform computation on the RDDs. Transformations follow Lazy evaluation. Basic RDD transformations are map, filter, flatmap and union. \n",
    "\n",
    "map() transformation applies a function to all elements in the RDD.\n",
    "\n",
    "filter transformation returns a new RDD with only the elements that pass the condition.\n",
    "\n",
    "flatmap is similar to map transformation except it returns multiple values for each element in the source RDD.\n",
    "\n",
    "Uninon transformation returns the union of one RDD with another RDD.\n",
    "\n",
    "Actions are the opretions that applied on RDDs to return a value after running a computation. The four basic actions are collect, take, first and count. \n",
    "\n",
    "Collect returns complete list of elements from the RDD.\n",
    "\n",
    "take(N) returns an array with the first N elements of the dataset.\n",
    "\n",
    "first prints the first element of the RDD.\n",
    "\n",
    "count return the number of elements in the RDD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map and Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "8\n",
      "27\n",
      "64\n",
      "125\n",
      "216\n",
      "343\n",
      "512\n",
      "729\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "numbRDD = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "cubeRDD = numbRDD.map(lambda x: x**3)\n",
    "\n",
    "numbers_all = cubeRDD.collect()\n",
    "for numb in numbers_all:\n",
    "    print(numb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of lines with the keyword Spark is 7\n",
      "[![buildstatus](https://travis-ci.org/holdenk/learning-spark-examples.svg?branch=master)](https://travis-ci.org/holdenk/learning-spark-examples)\n",
      "Examples for Learning Spark\n",
      "===============\n",
      "Examples for the Learning Spark book. These examples require a number of libraries and as such have long build files. We have also added a stand alone example with minimal dependencies and a small build file\n"
     ]
    }
   ],
   "source": [
    "fileRDD = sc.textFile(\"README.md\")\n",
    "fileRDD_filter = fileRDD.filter(lambda line: \"Spark\" in line)\n",
    "\n",
    "print(\"The total number of lines with the keyword Spark is\", fileRDD_filter.count())\n",
    "\n",
    "for line in fileRDD.take(4):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pair RDDs in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the real world datasets are generally key/value pairs. Each row is a key and maps to one or more values. PySpark provides a special data structure called pair RDDs for this kind of data. In pair RDDs, the key refers to the identifier, whereas value refers to the data. Pair RDDs can be created from a list of key-value tuple or from a regular RDD. The first step in creating pair RDDs is to get the data into key/value form. Since pair RDDs contain tuples, we need to pass functions that operate on key-value pairs. Some of the transformations of paired RDDs:\n",
    "    \n",
    "    reduceByKey(): Combine values with the same key\n",
    "    groupByKey(): Group values with the same key\n",
    "    sortByKey(): Return an RDD sorted by the same key\n",
    "    join(): Join two pair RDDs based on their key \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReduceBykey and Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 1 has 2 Counts\n",
      "Key 3 has 10 Counts\n",
      "Key 4 has 5 Counts\n"
     ]
    }
   ],
   "source": [
    "Rdd = sc.parallelize([(1,2),(3,4),(3,6),(4,5)])\n",
    "\n",
    "Rdd_reduced = Rdd.reduceByKey(lambda x, y: x+y)\n",
    "\n",
    "for num in Rdd_reduced.collect():\n",
    "    print(\"Key {} has {} Counts\".format(num[0], num[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SortByKey and Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 4 has 5 Counts\n",
      "Key 3 has 10 Counts\n",
      "Key 1 has 2 Counts\n"
     ]
    }
   ],
   "source": [
    "Rdd_reduced_sort = Rdd_reduced.sortByKey(ascending=False)\n",
    "\n",
    "for num in Rdd_reduced_sort.collect():\n",
    "    print(\"Key {} has {} Counts\".format(num[0], num[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced RDD Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduce action is used for aggregating the elements of a regular RDD. The function should be commutative and associative.\n",
    "\n",
    "It is not advisable to run collect action on RDDs because of the huge size of the data. In these cases, write data to a distributed storage systems such as HDFS or Amazon S3. saveAsTextFile action can be used to save RDD as a text file inside a particular directory, each partition as a separate file.\n",
    "\n",
    "With coalece method it can be saved as a single text file.\n",
    "\n",
    "countByKey is only available on RDDs of type (Key, Value). It counts the number of elements for each key.\n",
    "\n",
    "collectAsMap returns the key-value pairs in the RDD as a dictionary.\n",
    "\n",
    "These actions should only be used if the resulting data is expected to be small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountingBykeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of total is <class 'collections.defaultdict'>\n",
      "key 1 has 1 counts\n",
      "key 3 has 2 counts\n",
      "key 4 has 1 counts\n"
     ]
    }
   ],
   "source": [
    "Rdd = sc.parallelize([(1, 2), (3, 4), (3, 6), (4, 5)])\n",
    "\n",
    "total = Rdd.countByKey()\n",
    "\n",
    "print(\"The type of total is\", type(total))\n",
    "\n",
    "for k, v in total.items():\n",
    "    print(\"key\", k, \"has\", v, \"counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a base RDD and transform it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in splitRDD: 128576\n"
     ]
    }
   ],
   "source": [
    "baseRDD = sc.textFile(\"Complete_Shakespeare.txt\")\n",
    "\n",
    "splitRDD = baseRDD.flatMap(lambda x: x.split())\n",
    "print(\"Total number of words in splitRDD:\", splitRDD.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words and reduce the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['i',  'me',  'my',  'myself',  'we',  'our',  'ours',  'ourselves',  'you',  'your',  'yours',  'yourself',  'yourselves',  'he',  'him',  'his',  'himself',  'she',  'her',  'hers',  'herself',  'it',  'its',  'itself',  'they',  'them',  'their',  'theirs',  'themselves',  'what',  'which',  'who',  'whom',  'this',  'that',  'these',  'those',  'am',  'is',  'are',  'was',  'were',  'be',  'been',  'being',  'have',  'has',  'had',  'having',  'do',  'does',  'did',  'doing',  'a',  'an',  'the',  'and',  'but',  'if',  'or',  'because',  'as',  'until',  'while',  'of',  'at',  'by',  'for',  'with',  'about',  'against',  'between',  'into',  'through',  'during',  'before',  'after',  'above',  'below',  'to',  'from',  'up',  'down',  'in',  'out',  'on',  'off',  'over',  'under',  'again',  'further',  'then',  'once',  'here',  'there',  'when',  'where',  'why',  'how',  'all',  'any',  'both',  'each',  'few',  'more',  'most',  'other',  'some',  'such',  'no',  'nor',  'not',  'only',  'own',  'same',  'so',  'than',  'too',  'very',  'can',  'will',  'just',  'don',  'should',  'now']\n",
    "\n",
    "splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)\n",
    "\n",
    "splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))\n",
    "\n",
    "resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print word frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Project', 9)\n",
      "('Gutenberg', 7)\n",
      "('EBook', 1)\n",
      "('Complete', 3)\n",
      "('Works', 3)\n",
      "('William', 11)\n",
      "('Shakespeare,', 1)\n",
      "('Shakespeare', 12)\n",
      "('eBook', 2)\n",
      "('use', 38)\n",
      "\n",
      "(650, 'thou')\n",
      "(574, 'thy')\n",
      "(393, 'shall')\n",
      "(311, 'would')\n",
      "(295, 'good')\n",
      "(286, 'thee')\n",
      "(273, 'love')\n",
      "(269, 'Enter')\n",
      "(254, \"th'\")\n",
      "(225, 'make')\n"
     ]
    }
   ],
   "source": [
    "for word in resultRDD.take(10):\n",
    "    print(word)\n",
    "    \n",
    "resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\n",
    "resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)\n",
    "print()\n",
    "for word in resultRDD_swap_sort.take(10):\n",
    "    print(word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
