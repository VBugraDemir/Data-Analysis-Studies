{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Big Data analysis with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Big Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no single definition of Big Data since it is used quite differently.\n",
    "\n",
    "The 3 V's of Big Data:\n",
    "* Volume: Size of the data\n",
    "* Variety: Different sources and formats\n",
    "* Velocity: Speed of the data\n",
    "\n",
    "Some of the concepts of Big Data\n",
    "\n",
    "Clustered computing: Pooling resources of multiple macgines to complete jobs.\n",
    "\n",
    "    Parallel computing: Simultaneous computation.\n",
    "    Distributed computing: Nodes or networked computers that run jobs in parallel.\n",
    "    Batch precessing: Breaking data into smaller pieces and running each piece on an individual machine.\n",
    "    Real-time processing: Immediate processing of data.\n",
    "\n",
    "There are two popular frameworks for big data processing.\n",
    "\n",
    "    Hadoop/MapReduce: It is open source and scalable framework for batch data.\n",
    "    Apache Spark: Parallel framework for storing and processing of big data across clustered computers. It is open source and is suited for both batch and  real-time data processing. \n",
    "\n",
    "Spark distribute data and computation across multiple computers. Runs most computations in memory and provides better performance for applications like interactive data mining. \n",
    "\n",
    "Spark can be run on two modes:\n",
    "\n",
    "    Local mode: Single machine (laptop). It is very convenient for testing, debugging and demonstration purposes.\n",
    "    Cluster mode: Spark is run on a cluster. Set of pre-defined machines. It is good for production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark: Spark with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark helps data scientists interface with Spark data structures in Apache Spark and python. In order to interact with Spark using PySpark shell, you need an entry point. SparkContext is an entry point to interact with underlying Spark functionality. An entry point is a way of connecting to Spark cluster.\n",
    "\n",
    "You can load data into PySpark using SparkContext by two different methods. SparkContext's parallilize method on a list. Or SparkContext's textFile method on a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"pyspark-shell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The version of Spark Context in the PySpark shell is 3.1.2\n",
      "\n",
      "The Python version of Spark Context in the PySpark shell is 3.8\n",
      "\n",
      "The master of Spark Context in the PySpark shell is local\n"
     ]
    }
   ],
   "source": [
    "print(\"The version of Spark Context in the PySpark shell is\", sc.version)\n",
    "print()\n",
    "print(\"The Python version of Spark Context in the PySpark shell is\", sc.pythonVer)\n",
    "print()\n",
    "print(\"The master of Spark Context in the PySpark shell is\", sc.master)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Use of PySpark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "numb = range(1, 101)\n",
    "\n",
    "spark_data = sc.parallelize(numb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data in PySpark shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of functional programming in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda functions are powerful and quite efficient with map() and filter(). Lambda functions are anonymous. They return the functions instead of names.\n",
    "Most of the times lambda functions are used with built-in functions like map and filter. map() function takes a function and a list and retuns a new list which contains items returned by that function for each item. filter() function takes a functions and a list and retuns a new list for which the function evaluates as true.\n",
    "\n",
    "General syntax of filter and map:\n",
    "\n",
    "    filter(function, list)\n",
    "    map(function, list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input list is [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "The squared numbers are [1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n"
     ]
    }
   ],
   "source": [
    "my_list =  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "print(\"Input list is\", my_list)\n",
    "\n",
    "squared_list_lambda = list(map(lambda x: x**2, my_list))\n",
    "\n",
    "print(\"The squared numbers are\", squared_list_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of lambda() with filter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input list is: [10, 21, 31, 40, 51, 60, 72, 80, 93, 101]\n",
      "Numbers divisible by 10 are: [10, 40, 60, 80]\n"
     ]
    }
   ],
   "source": [
    "my_list2 = [10, 21, 31, 40, 51, 60, 72, 80, 93, 101]\n",
    "print(\"Input list is:\", my_list2)\n",
    "\n",
    "filtered_list = list(filter(lambda x: x%10 == 0, my_list2))\n",
    "print(\"Numbers divisible by 10 are:\", filtered_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
