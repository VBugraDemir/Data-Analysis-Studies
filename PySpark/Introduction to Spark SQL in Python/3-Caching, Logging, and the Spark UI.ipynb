{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"pyspark-shell\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching, Logging, and the Spark UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cachimg is keeping data in memory so that it does not have to be refetched or recalculated each time it is used. To cache a dataframe use df.cache() to uncache it use df.unpersist(). You can check for a dataframe if it is cached or not by using df.is_cached. df.storageLevel specifies 5 details about how it is cached:\n",
    "\n",
    "* useDisk\n",
    "* useMemory\n",
    "* useOffHeap\n",
    "* deserialized\n",
    "* replication\n",
    "useDisk specifies whether to move some or all the dataframe to disk if it needed to free up memory.\n",
    "\n",
    "useMemory specifies whether to keep the data in memory.\n",
    "\n",
    "useOffHeap tells Spark to use off-heap storage insted of on-heap memory. Off-heap storage is slightly slower than on-heap but still faster than disk. \n",
    "\n",
    "Deserialized True is faster but uses more memory. Serialized data is more space-efficient but slower to read than deserialized data.\n",
    "\n",
    "Replication is used to tell Spark to replicate data on multiple nodes. \n",
    "\n",
    "Reading the dataframe from disk cache is slower than reading it from memory, but can still be faster than recreating from scratch.\n",
    "\n",
    "df.cache() is the same as df.persist().\n",
    "\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "spark.catalog.isCached(tableName=\"df\") tells you whether a table has been cached. \n",
    "\n",
    "You can cache a table by using \n",
    "spark.catalog.cacheTable(\"df\")\n",
    "spark.catalog.isCached(tableName=\"df\")\n",
    "\n",
    "to uncache\n",
    "spark.catalog.uncacheTable(\"df\")\n",
    "\n",
    "to remove all cached tables\n",
    "spark.catalog.clearCache()\n",
    "    \n",
    "Caching incurs a cost. Caching everything slows things down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practicing caching: part 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see functions in detail use inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1_1st : 1.4s\n",
      "df1_2nd : 0.1s\n",
      "df2_1st : 0.2s\n",
      "df2_2nd : 0.1s\n",
      "Overall elapsed : 1.8\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def prep(df1, df2):\n",
    "    global begin\n",
    "    df1.unpersist()\n",
    "    df2.unpersist()\n",
    "    begin = time.time()\n",
    "\n",
    "def print_elapsed():\n",
    "    print(\"Overall elapsed : %.1f\" % (time.time() - begin))\n",
    "\n",
    "def run(df,name, elapsed=False):\n",
    "    start=time.time()\n",
    "    df.count()\n",
    "    print(\"%s : %.1fs\" % (name, (time.time()-start)))\n",
    "    if elapsed:\n",
    "        print_elapsed()\n",
    "        \n",
    "df1 = spark.read.load(\"sherlock1.parquet\").filter(\"id< 606568\")\n",
    "df2 = spark.read.load(\"sherlock1.parquet\").filter(\"id< 499691\")\n",
    "\n",
    "prep(df1, df2) \n",
    "df1.cache()\n",
    "\n",
    "run(df1, \"df1_1st\") \n",
    "run(df1, \"df1_2nd\")\n",
    "run(df2, \"df2_1st\")\n",
    "run(df2, \"df2_2nd\", elapsed=True)\n",
    "\n",
    "print(df1.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practicing caching: the SQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1_1st : 0.1s\n",
      "df1_2nd : 0.1s\n",
      "df2_1st : 1.2s\n",
      "df2_2nd : 0.1s\n",
      "Overall elapsed : 1.5\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "prep(df1, df2) # unpersisting\n",
    "df2.persist(storageLevel=pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "run(df1, \"df1_1st\") \n",
    "run(df1, \"df1_2nd\") \n",
    "run(df2, \"df2_1st\") \n",
    "run(df2, \"df2_2nd\", elapsed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching and uncaching tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables:\n",
      " [Table(name='df1', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='df2', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n",
      "table1 is cached:  True\n",
      "table1 is cached:  False\n"
     ]
    }
   ],
   "source": [
    "print(\"Tables:\\n\", spark.catalog.listTables())\n",
    "\n",
    "spark.catalog.cacheTable(\"df1\")\n",
    "print(\"table1 is cached: \", spark.catalog.isCached(\"df1\"))\n",
    "\n",
    "spark.catalog.uncacheTable(\"df1\")\n",
    "print(\"table1 is cached: \", spark.catalog.isCached(\"df1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Spark UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spark UI is a web intergace to inspect Spark execution.\n",
    "\n",
    "**Spark Task** is a unit of execution that runs on a single cpu. \n",
    "\n",
    "**Spark Stage** a group of tasks that perform the same computation in parallel, each task typically running on a different  subset of data. \n",
    "\n",
    "**Spark Job** computaion triggered by an action, sliced into one or more stages.\n",
    "\n",
    "The Spark UI also shows casche, settings and SQL queries.\n",
    "\n",
    "spark.catalog.dropTempView(\"table1\") removes the temporary table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions on large dataframes can be costly to calculate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG,\n",
    "                    format='%(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG - text_df columns: ['word', 'id']\n",
      "DEBUG - Command to send: c\n",
      "o70\n",
      "isCached\n",
      "sdf1\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ybfalse\n",
      "INFO - table1 is cached: False\n",
      "DEBUG - Command to send: c\n",
      "o63\n",
      "limit\n",
      "i1\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro129\n",
      "DEBUG - Command to send: c\n",
      "o13\n",
      "setCallSite\n",
      "sfirst at <ipython-input-52-eb8c695700e4>:5\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yv\n",
      "DEBUG - Command to send: c\n",
      "o129\n",
      "collectToPython\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yto130\n",
      "DEBUG - Command to send: c\n",
      "o13\n",
      "setCallSite\n",
      "n\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yv\n",
      "DEBUG - Command to send: a\n",
      "e\n",
      "o130\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yi3\n",
      "DEBUG - Command to send: a\n",
      "g\n",
      "o130\n",
      "i0\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yi51320\n",
      "DEBUG - Command to send: a\n",
      "e\n",
      "o130\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yi3\n",
      "DEBUG - Command to send: a\n",
      "g\n",
      "o130\n",
      "i1\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ys327509f39134e0d1cc21b0a88d3ff879619137dae26d25ba790150600c174a14\n",
      "DEBUG - Command to send: m\n",
      "d\n",
      "o130\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yv\n",
      "WARNING - The first row of text_df:\n",
      " Row(word='the', id=0)\n",
      "DEBUG - Command to send: r\n",
      "u\n",
      "functions\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ycorg.apache.spark.sql.functions\n",
      "DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.functions\n",
      "col\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ym\n",
      "DEBUG - Command to send: c\n",
      "z:org.apache.spark.sql.functions\n",
      "col\n",
      "sword\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro131\n",
      "DEBUG - Command to send: r\n",
      "u\n",
      "PythonUtils\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ycorg.apache.spark.api.python.PythonUtils\n",
      "DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.api.python.PythonUtils\n",
      "toSeq\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ym\n",
      "DEBUG - Command to send: i\n",
      "java.util.ArrayList\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ylo132\n",
      "DEBUG - Command to send: c\n",
      "o132\n",
      "add\n",
      "ro131\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ybtrue\n",
      "DEBUG - Command to send: c\n",
      "z:org.apache.spark.api.python.PythonUtils\n",
      "toSeq\n",
      "ro132\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro133\n",
      "DEBUG - Command to send: m\n",
      "d\n",
      "o132\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yv\n",
      "DEBUG - Command to send: c\n",
      "o27\n",
      "select\n",
      "ro133\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro134\n",
      "DEBUG - Command to send: c\n",
      "o24\n",
      "sessionState\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro135\n",
      "DEBUG - Command to send: c\n",
      "o135\n",
      "conf\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro136\n",
      "DEBUG - Command to send: c\n",
      "o136\n",
      "isReplEagerEvalEnabled\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ybfalse\n",
      "DEBUG - Command to send: c\n",
      "o134\n",
      "schema\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro137\n",
      "DEBUG - Command to send: c\n",
      "o137\n",
      "json\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ys{\"type\":\"struct\",\"fields\":[{\"name\":\"word\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}\n",
      "ERROR - Selected columns: DataFrame[word: string]\n"
     ]
    }
   ],
   "source": [
    "logging.debug(\"text_df columns: %s\", df1.columns)\n",
    "\n",
    "logging.info(\"table1 is cached: %s\", spark.catalog.isCached(tableName=\"df1\"))\n",
    "\n",
    "logging.warning(\"The first row of text_df:\\n %s\", df1.first())\n",
    "\n",
    "logging.error(\"Selected columns: %s\", df.select(\"word\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice logging 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG - df1 columns: ['word', 'id']\n",
      "DEBUG - Command to send: c\n",
      "o70\n",
      "isCached\n",
      "sdf1\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ybfalse\n",
      "INFO - df1 is cached: False\n",
      "DEBUG - Command to send: r\n",
      "u\n",
      "functions\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ycorg.apache.spark.sql.functions\n",
      "DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.functions\n",
      "col\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ym\n",
      "DEBUG - Command to send: c\n",
      "z:org.apache.spark.sql.functions\n",
      "col\n",
      "sid\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro138\n",
      "DEBUG - Command to send: r\n",
      "u\n",
      "functions\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ycorg.apache.spark.sql.functions\n",
      "DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.sql.functions\n",
      "col\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ym\n",
      "DEBUG - Command to send: c\n",
      "z:org.apache.spark.sql.functions\n",
      "col\n",
      "sword\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro139\n",
      "DEBUG - Command to send: r\n",
      "u\n",
      "PythonUtils\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ycorg.apache.spark.api.python.PythonUtils\n",
      "DEBUG - Command to send: r\n",
      "m\n",
      "org.apache.spark.api.python.PythonUtils\n",
      "toSeq\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ym\n",
      "DEBUG - Command to send: i\n",
      "java.util.ArrayList\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ylo140\n",
      "DEBUG - Command to send: c\n",
      "o140\n",
      "add\n",
      "ro138\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ybtrue\n",
      "DEBUG - Command to send: c\n",
      "o140\n",
      "add\n",
      "ro139\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ybtrue\n",
      "DEBUG - Command to send: c\n",
      "z:org.apache.spark.api.python.PythonUtils\n",
      "toSeq\n",
      "ro140\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro141\n",
      "DEBUG - Command to send: m\n",
      "d\n",
      "o140\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yv\n",
      "DEBUG - Command to send: c\n",
      "o63\n",
      "select\n",
      "ro141\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro142\n",
      "DEBUG - Command to send: c\n",
      "o24\n",
      "sessionState\n",
      "e\n",
      "\n",
      "DEBUG - Command to send: A\n",
      "9a1279552fa4d72bec3b14ff3be40512df1f3adab772a17ed769a50d14c32b47\n",
      "\n",
      "DEBUG - Answer received: !yv\n",
      "DEBUG - Command to send: m\n",
      "d\n",
      "o127\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yv\n",
      "DEBUG - Command to send: m\n",
      "d\n",
      "o129\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yv\n",
      "DEBUG - Command to send: m\n",
      "d\n",
      "o131\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yv\n",
      "DEBUG - Command to send: m\n",
      "d\n",
      "o133\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yv\n",
      "DEBUG - Command to send: m\n",
      "d\n",
      "o134\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yv\n",
      "DEBUG - Command to send: m\n",
      "d\n",
      "o135\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yv\n",
      "DEBUG - Command to send: m\n",
      "d\n",
      "o136\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yv\n",
      "DEBUG - Command to send: m\n",
      "d\n",
      "o137\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yv\n",
      "DEBUG - Command to send: m\n",
      "d\n",
      "o138\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yv\n",
      "DEBUG - Command to send: m\n",
      "d\n",
      "o139\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yv\n",
      "DEBUG - Command to send: m\n",
      "d\n",
      "o141\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yv\n",
      "DEBUG - Answer received: !yro143\n",
      "DEBUG - Command to send: c\n",
      "o143\n",
      "conf\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro144\n",
      "DEBUG - Command to send: c\n",
      "o144\n",
      "isReplEagerEvalEnabled\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ybfalse\n",
      "DEBUG - Command to send: c\n",
      "o142\n",
      "schema\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro145\n",
      "DEBUG - Command to send: c\n",
      "o145\n",
      "json\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ys{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"word\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}\n",
      "ERROR - Selected columns: DataFrame[id: bigint, word: string]\n",
      "DEBUG - Command to send: c\n",
      "o24\n",
      "sql\n",
      "sshow tables\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro146\n",
      "DEBUG - Command to send: c\n",
      "o13\n",
      "setCallSite\n",
      "scollect at <ipython-input-53-856cb66efde8>:6\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yv\n",
      "DEBUG - Command to send: c\n",
      "o146\n",
      "collectToPython\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yto147\n",
      "DEBUG - Command to send: c\n",
      "o13\n",
      "setCallSite\n",
      "n\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yv\n",
      "DEBUG - Command to send: a\n",
      "e\n",
      "o147\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yi3\n",
      "DEBUG - Command to send: a\n",
      "g\n",
      "o147\n",
      "i0\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yi62464\n",
      "DEBUG - Command to send: a\n",
      "e\n",
      "o147\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yi3\n",
      "DEBUG - Command to send: a\n",
      "g\n",
      "o147\n",
      "i1\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ys327509f39134e0d1cc21b0a88d3ff879619137dae26d25ba790150600c174a14\n",
      "DEBUG - Command to send: m\n",
      "d\n",
      "o147\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yv\n",
      "INFO - Tables: [Row(database='', tableName='df1', isTemporary=True), Row(database='', tableName='df2', isTemporary=True)]\n",
      "DEBUG - Command to send: c\n",
      "o24\n",
      "sql\n",
      "sSELECT * FROM df1 limit 1\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro148\n",
      "DEBUG - Command to send: c\n",
      "o24\n",
      "sessionState\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro149\n",
      "DEBUG - Command to send: c\n",
      "o149\n",
      "conf\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro150\n",
      "DEBUG - Command to send: c\n",
      "o150\n",
      "isReplEagerEvalEnabled\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ybfalse\n",
      "DEBUG - Command to send: c\n",
      "o148\n",
      "schema\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !yro151\n",
      "DEBUG - Command to send: c\n",
      "o151\n",
      "json\n",
      "e\n",
      "\n",
      "DEBUG - Answer received: !ys{\"type\":\"struct\",\"fields\":[{\"name\":\"word\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"id\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]}\n",
      "DEBUG - First row: DataFrame[word: string, id: bigint]\n"
     ]
    }
   ],
   "source": [
    "# statements that triggert df1 are commented out\n",
    "logging.debug(\"df1 columns: %s\", df1.columns)\n",
    "logging.info(\"df1 is cached: %s\", spark.catalog.isCached(tableName=\"df1\"))\n",
    "# logging.warning(\"The first row of df1: %s\", df1.first())\n",
    "logging.error(\"Selected columns: %s\", df1.select(\"id\", \"word\"))\n",
    "logging.info(\"Tables: %s\", spark.sql(\"show tables\").collect())\n",
    "logging.debug(\"First row: %s\", spark.sql(\"SELECT * FROM df1 limit 1\"))\n",
    "# logging.debug(\"Count: %s\", spark.sql(\"SELECT COUNT(*) AS count FROM df1\").collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query plans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With query plans we can see how the data was obtained and from where.\n",
    "\n",
    "spark.sql(\"EXPLAIN SELECT * FROM df\").show(truncate=False)\n",
    "\n",
    "df.explain() formats results to be easier to read.\n",
    "\n",
    "Reading from the bottom-up tells us the steps in order from the first step first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice query plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(id#202L) AND (id#202L < 606568))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [word#201,id#202L] Batched: true, DataFilters: [isnotnull(id#202L), (id#202L < 606568)], Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/Buğra/Datacamp-jupyter_notebook/PySpark/Introduction to Spark SQ..., PartitionFilters: [], PushedFilters: [IsNotNull(id), LessThan(id,606568)], ReadSchema: struct<word:string,id:bigint>\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[], functions=[count(1)])\n",
      "+- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#1517]\n",
      "   +- *(1) HashAggregate(keys=[], functions=[partial_count(1)])\n",
      "      +- *(1) Project\n",
      "         +- *(1) Filter (isnotnull(id#202L) AND (id#202L < 606568))\n",
      "            +- *(1) ColumnarToRow\n",
      "               +- FileScan parquet [id#202L] Batched: true, DataFilters: [isnotnull(id#202L), (id#202L < 606568)], Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/Buğra/Datacamp-jupyter_notebook/PySpark/Introduction to Spark SQ..., PartitionFilters: [], PushedFilters: [IsNotNull(id), LessThan(id,606568)], ReadSchema: struct<id:bigint>\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) HashAggregate(keys=[], functions=[count(distinct word#201)])\n",
      "+- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#1567]\n",
      "   +- *(2) HashAggregate(keys=[], functions=[partial_count(distinct word#201)])\n",
      "      +- *(2) HashAggregate(keys=[word#201], functions=[])\n",
      "         +- Exchange hashpartitioning(word#201, 200), ENSURE_REQUIREMENTS, [id=#1562]\n",
      "            +- *(1) HashAggregate(keys=[word#201], functions=[])\n",
      "               +- *(1) Project [word#201]\n",
      "                  +- *(1) Filter (isnotnull(id#202L) AND (id#202L < 606568))\n",
      "                     +- *(1) ColumnarToRow\n",
      "                        +- FileScan parquet [word#201,id#202L] Batched: true, DataFilters: [isnotnull(id#202L), (id#202L < 606568)], Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/Buğra/Datacamp-jupyter_notebook/PySpark/Introduction to Spark SQ..., PartitionFilters: [], PushedFilters: [IsNotNull(id), LessThan(id,606568)], ReadSchema: struct<word:string,id:bigint>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.explain()\n",
    "\n",
    "spark.sql(\"SELECT COUNT(*) AS count FROM df1\").explain()\n",
    "\n",
    "spark.sql(\"SELECT COUNT(DISTINCT word) AS words FROM df1\").explain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
