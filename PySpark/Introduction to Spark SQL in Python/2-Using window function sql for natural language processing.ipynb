{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using window function sql for natural language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading natural language text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load parquet file which is a Hadoop file format to store data structures use spark.read.load(...parquet).\n",
    "\n",
    "regexp_replace replaces values that matches a pattern.\n",
    "\n",
    "df = df1.select(regexp_replace('value', 'Mr\\.', 'Mr').alias('v')) to make Mr. Holmes > Mr Holmes\n",
    "\n",
    "split operation seperates a string into individual tokens. Splitting on unwanted symbols in addition to spaces discards the unwanted symbols. (df.select(split(\"name\",\"[ ]\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"pyspark-shell\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a dataframe from a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from urllib.request import urlretrieve\n",
    "# url = \"https://assets.datacamp.com/production/repositories/3937/datasets/213ca262bf6af12428d42842848464565f3d5504/sherlock.txt\"\n",
    "# data = urlretrieve(url, \"sherlock.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_parquet(\"sherlock_sentences.parquet\", engine='pyarrow', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              clause|\n",
      "+--------------------+\n",
      "|               title|\n",
      "|the adventures of...|\n",
      "|sir arthur conan ...|\n",
      "|          march 1999|\n",
      "|          ebook 1661|\n",
      "|most recently upd...|\n",
      "|    november 29 2002|\n",
      "|             edition|\n",
      "|         12 language|\n",
      "|english character...|\n",
      "|               ascii|\n",
      "|start of the proj...|\n",
      "|additional editin...|\n",
      "|the adventures of...|\n",
      "|a scandal in bohe...|\n",
      "|             the red|\n",
      "|   headed league iii|\n",
      "|a case of identit...|\n",
      "|the boscombe vall...|\n",
      "|the five orange p...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.load(\"sherlock_sentences.parquet\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|              clause| id|\n",
      "+--------------------+---+\n",
      "|               title|  0|\n",
      "|the adventures of...|  1|\n",
      "|sir arthur conan ...|  2|\n",
      "|          march 1999|  3|\n",
      "|          ebook 1661|  4|\n",
      "|most recently upd...|  5|\n",
      "|    november 29 2002|  6|\n",
      "|             edition|  7|\n",
      "|         12 language|  8|\n",
      "|english character...|  9|\n",
      "|               ascii| 10|\n",
      "|start of the proj...| 11|\n",
      "|additional editin...| 12|\n",
      "|the adventures of...| 13|\n",
      "|a scandal in bohe...| 14|\n",
      "|             the red| 15|\n",
      "|   headed league iii| 16|\n",
      "|a case of identit...| 17|\n",
      "|the boscombe vall...| 18|\n",
      "|the five orange p...| 19|\n",
      "+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "df = df.withColumn(\"id\", monotonically_increasing_id())\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------+---+\n",
      "|clause                                                  |id |\n",
      "+--------------------------------------------------------+---+\n",
      "|i answered                                              |71 |\n",
      "|indeed i should have thought a little more              |72 |\n",
      "|just a trifle more i fancy watson                       |73 |\n",
      "|and in practice again i observe                         |74 |\n",
      "|you did not tell me that you intended to go into harness|75 |\n",
      "+--------------------------------------------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where('id > 70').show(5, truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and explode a text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT * FROM df LIMIT 100\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+\n",
      "|words                                          |\n",
      "+-----------------------------------------------+\n",
      "|[title]                                        |\n",
      "|[the, adventures, of, sherlock, holmes, author]|\n",
      "|[sir, arthur, conan, doyle, release, date]     |\n",
      "|[march, 1999]                                  |\n",
      "|[ebook, 1661]                                  |\n",
      "+-----------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+\n",
      "|      word|\n",
      "+----------+\n",
      "|     title|\n",
      "|       the|\n",
      "|adventures|\n",
      "|        of|\n",
      "|  sherlock|\n",
      "|    holmes|\n",
      "|    author|\n",
      "|       sir|\n",
      "|    arthur|\n",
      "|     conan|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Number of rows:  1279\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "\n",
    "split_df = df.select(split(\"clause\", \" \").alias(\"words\"))\n",
    "split_df.show(5, truncate=False)\n",
    "\n",
    "exploded_df = split_df.select(explode(\"words\").alias(\"word\"))\n",
    "exploded_df.show(10)\n",
    "\n",
    "print(\"Number of rows: \", exploded_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving window analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating 3 tuple by using sliding windows.\n",
    "\n",
    "Properly repartitioning data allows Spark to parallelize operations more efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Creating context window feature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+----+--------------------+\n",
      "|     word|   id|part|               title|\n",
      "+---------+-----+----+--------------------+\n",
      "|      xii|95166|  12|Sherlock Chapter XII|\n",
      "|      the|95167|  12|Sherlock Chapter XII|\n",
      "|adventure|95168|  12|Sherlock Chapter XII|\n",
      "|       of|95169|  12|Sherlock Chapter XII|\n",
      "|      the|95170|  12|Sherlock Chapter XII|\n",
      "|   copper|95171|  12|Sherlock Chapter XII|\n",
      "|  beeches|95172|  12|Sherlock Chapter XII|\n",
      "|       to|95173|  12|Sherlock Chapter XII|\n",
      "|      the|95174|  12|Sherlock Chapter XII|\n",
      "|      man|95175|  12|Sherlock Chapter XII|\n",
      "|      who|95176|  12|Sherlock Chapter XII|\n",
      "|    loves|95177|  12|Sherlock Chapter XII|\n",
      "|      art|95178|  12|Sherlock Chapter XII|\n",
      "|      for|95179|  12|Sherlock Chapter XII|\n",
      "|      its|95180|  12|Sherlock Chapter XII|\n",
      "|      own|95181|  12|Sherlock Chapter XII|\n",
      "|     sake|95182|  12|Sherlock Chapter XII|\n",
      "| remarked|95183|  12|Sherlock Chapter XII|\n",
      "| sherlock|95184|  12|Sherlock Chapter XII|\n",
      "|   holmes|95185|  12|Sherlock Chapter XII|\n",
      "+---------+-----+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df = spark.read.load(\"sherlock1.parquet\")\n",
    "df2 = df.filter(\"id > 95165\")\n",
    "df2 = df2.withColumn(\"part\", lit(12))\n",
    "df2 = df2.withColumn(\"title\", lit(\"Sherlock Chapter XII\"))\n",
    "df2.createOrReplaceTempView(\"df2\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+---------+---------+---------+---------+\n",
      "|part|       w1|       w2|       w3|       w4|       w5|\n",
      "+----+---------+---------+---------+---------+---------+\n",
      "|  12|     null|     null|      xii|      the|adventure|\n",
      "|  12|     null|      xii|      the|adventure|       of|\n",
      "|  12|      xii|      the|adventure|       of|      the|\n",
      "|  12|      the|adventure|       of|      the|   copper|\n",
      "|  12|adventure|       of|      the|   copper|  beeches|\n",
      "|  12|       of|      the|   copper|  beeches|       to|\n",
      "|  12|      the|   copper|  beeches|       to|      the|\n",
      "|  12|   copper|  beeches|       to|      the|      man|\n",
      "|  12|  beeches|       to|      the|      man|      who|\n",
      "|  12|       to|      the|      man|      who|    loves|\n",
      "+----+---------+---------+---------+---------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"SELECT part, LAG(word, 2) OVER(PARTITION BY part ORDER BY id) AS w1,\n",
    "             LAG(word, 1) OVER(PARTITION BY part ORDER BY id) AS w2,\n",
    "             word AS w3,\n",
    "             LEAD(word, 1) OVER(PARTITION BY part ORDER BY id) AS w4,\n",
    "             LEAD(word, 2) OVER(PARTITION BY part ORDER BY id) AS w5\n",
    "             FROM df2\"\"\"\n",
    "\n",
    "spark.sql(query).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repartitioning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe is currently in a single partition. Suppose that you know that the upcoming processing steps are going to be grouping the data on chapters. Processing the data will be most efficient if each chapter stays within a single machine. To avoid unnecessary shuffling of the data from one machine to another, let's repartition the dataframe into one partition per chapter, using the repartition and getNumPartitions commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "df3 = df.withColumn(\"chapter\", when(df.id < 9260, \"Sherlock Chapter I\")\n",
    "                    .when(df.id < 18520, \"Sherlock Chapter II\")\n",
    "                    .when(df.id < 27780, \"Sherlock Chapter III\")\n",
    "                    .when(df.id < 37040, \"Sherlock Chapter IV\")\n",
    "                    .when(df.id < 46300, \"Sherlock Chapter V\")\n",
    "                    .when(df.id < 55560, \"Sherlock Chapter VI\")\n",
    "                    .when(df.id < 64820, \"Sherlock Chapter VII\")\n",
    "                    .when(df.id < 74080, \"Sherlock Chapter VIII\")\n",
    "                    .when(df.id < 83340, \"Sherlock Chapter IX\")\n",
    "                    .when(df.id < 92600, \"Sherlock Chapter X\")\n",
    "                    .when(df.id < 101860, \"Sherlock Chapter XI\")\n",
    "                    .otherwise(\"Sherlock Chapter XII\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|chapter              |\n",
      "+---------------------+\n",
      "|Sherlock Chapter I   |\n",
      "|Sherlock Chapter II  |\n",
      "|Sherlock Chapter III |\n",
      "|Sherlock Chapter IV  |\n",
      "|Sherlock Chapter IX  |\n",
      "|Sherlock Chapter V   |\n",
      "|Sherlock Chapter VI  |\n",
      "|Sherlock Chapter VII |\n",
      "|Sherlock Chapter VIII|\n",
      "|Sherlock Chapter X   |\n",
      "|Sherlock Chapter XI  |\n",
      "|Sherlock Chapter XII |\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.select(\"chapter\").distinct().sort(\"chapter\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-------------------+\n",
      "|    word|   id|            chapter|\n",
      "+--------+-----+-------------------+\n",
      "|   which|27780|Sherlock Chapter IV|\n",
      "|followed|27781|Sherlock Chapter IV|\n",
      "|     the|27782|Sherlock Chapter IV|\n",
      "|coroner:|27783|Sherlock Chapter IV|\n",
      "|    that|27784|Sherlock Chapter IV|\n",
      "|      is|27785|Sherlock Chapter IV|\n",
      "|     for|27786|Sherlock Chapter IV|\n",
      "|     the|27787|Sherlock Chapter IV|\n",
      "|   court|27788|Sherlock Chapter IV|\n",
      "|      to|27789|Sherlock Chapter IV|\n",
      "|  decide|27790|Sherlock Chapter IV|\n",
      "|       i|27791|Sherlock Chapter IV|\n",
      "|    need|27792|Sherlock Chapter IV|\n",
      "|     not|27793|Sherlock Chapter IV|\n",
      "|   point|27794|Sherlock Chapter IV|\n",
      "|     out|27795|Sherlock Chapter IV|\n",
      "|      to|27796|Sherlock Chapter IV|\n",
      "|     you|27797|Sherlock Chapter IV|\n",
      "|    that|27798|Sherlock Chapter IV|\n",
      "|    your|27799|Sherlock Chapter IV|\n",
      "+--------+-----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repart_df = df3.repartition(12,\"chapter\")\n",
    "repart_df.show()\n",
    "repart_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common word sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create training sets for predictive models to predict a word from previous wors in a sequence. Categorical data generally have no logical order when they do they are called ordinal data.\n",
    "\n",
    "You can determine what words tend to appear together by sequence analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------+--------+------+-----+\n",
      "|   w1|        w2|    w3|      w4|    w5|count|\n",
      "+-----+----------+------+--------+------+-----+\n",
      "|   in|       the|  case|      of|   the|    4|\n",
      "|  the|adventures|    of|sherlock|holmes|    4|\n",
      "|  the|    church|    of|      st|monica|    3|\n",
      "| what|        do|   you|    make|    of|    3|\n",
      "|  the|       man|   who| entered|   was|    3|\n",
      "|dying| reference|    to|       a|   rat|    3|\n",
      "|    i|        am|afraid|    that|     i|    3|\n",
      "|    i|     think|  that|      it|    is|    3|\n",
      "|   in|       his| chair|    with|   his|    3|\n",
      "|    i|      rang|   the|    bell|   and|    3|\n",
      "+-----+----------+------+--------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"SELECT w1, w2, w3, w4, w5, COUNT(*) AS count FROM(SELECT word AS w1,\n",
    "                                                LEAD(word, 1) OVER(PARTITION BY chapter ORDER BY id) AS w2,\n",
    "                                                LEAD(word, 2) OVER(PARTITION BY chapter ORDER BY id) AS w3,\n",
    "                                                LEAD(word, 3) OVER(PARTITION BY chapter ORDER BY id) AS w4,\n",
    "                                                LEAD(word, 4) OVER(PARTITION BY chapter ORDER BY id) AS w5 FROM df3\n",
    "                                                WHERE id < 41729)\n",
    "            GROUP BY w1, w2, w3, w4, w5\n",
    "            ORDER BY count DESC\n",
    "            LIMIT 10\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique 5-tuples in sorted order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---------+------+-----+\n",
      "|        w1|    w2|       w3|    w4|   w5|\n",
      "+----------+------+---------+------+-----+\n",
      "|   zealand| stock|   paying|     4|  1/4|\n",
      "|   youwill|   see|     your|   pal|again|\n",
      "|   youwill|    do|     come|  come| what|\n",
      "|     youth|though|   comely|    to| look|\n",
      "|     youth|    in|       an|ulster|  who|\n",
      "|     youth|either|       it|     s| hard|\n",
      "|     youth| asked| sherlock|holmes|  his|\n",
      "|yourselves|  that|       my|  hair|   is|\n",
      "|yourselves|behind|    those|  then| when|\n",
      "|  yourself|  your|household|   and|  the|\n",
      "+----------+------+---------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"SELECT DISTINCT w1, w2, w3, w4, w5 FROM(SELECT word AS w1,\n",
    "                                              LEAD(word, 1) OVER(PARTITION BY chapter ORDER BY id) AS w2,\n",
    "                                              LEAD(word, 2) OVER(PARTITION BY chapter ORDER BY id) AS w3,\n",
    "                                              LEAD(word, 3) OVER(PARTITION BY chapter ORDER BY id) AS w4,\n",
    "                                              LEAD(word, 4) OVER(PARTITION BY chapter ORDER BY id) AS w5 FROM df3\n",
    "                                              WHERE id < 34389)\n",
    "            ORDER BY w1 DESC, w2 DESC, w3 DESC, w4 DESC, w5 DESC\n",
    "            LIMIT 10\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most frequent 3-tuples per chapter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+------+-----+-----+\n",
      "|             chapter|     w1|    w2|   w3|count|\n",
      "+--------------------+-------+------+-----+-----+\n",
      "|  Sherlock Chapter I|    one|    of|  the|    7|\n",
      "| Sherlock Chapter II|    one|    of|  the|    7|\n",
      "|Sherlock Chapter III|     mr|hosmer|angel|   13|\n",
      "| Sherlock Chapter IV|   that|    he|  was|    7|\n",
      "| Sherlock Chapter IX|   lord|    st|simon|   19|\n",
      "|  Sherlock Chapter V|    one|    of|  the|    7|\n",
      "| Sherlock Chapter VI|neville|    st|clair|    9|\n",
      "|Sherlock Chapter VII|     at|   the| time|    7|\n",
      "|Sherlock Chapter ...|   that|    it|  was|    9|\n",
      "|  Sherlock Chapter X|   lord|    st|simon|    9|\n",
      "| Sherlock Chapter XI|     to|    be|    a|    9|\n",
      "|Sherlock Chapter XII|    one|    of|  the|  327|\n",
      "+--------------------+-------+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subquery = \"\"\"\n",
    "SELECT chapter, w1, w2, w3, COUNT(*) as count\n",
    "FROM\n",
    "(\n",
    "    SELECT\n",
    "    chapter,\n",
    "    word AS w1,\n",
    "    LEAD(word, 1) OVER(PARTITION BY chapter ORDER BY id ) AS w2,\n",
    "    LEAD(word, 2) OVER(PARTITION BY chapter ORDER BY id ) AS w3\n",
    "    FROM df3\n",
    ")\n",
    "GROUP BY chapter, w1, w2, w3\n",
    "ORDER BY chapter, count DESC\n",
    "\"\"\"\n",
    "\n",
    "df4 = spark.sql(subquery)\n",
    "df4.createOrReplaceTempView(\"df4\")\n",
    "query = (\"\"\"SELECT chapter, w1, w2, w3, count FROM(SELECT chapter, \n",
    "                                                ROW_NUMBER() OVER(PARTITION BY chapter ORDER BY count DESC) AS row,\n",
    "                                                w1, w2, w3, count\n",
    "                                                FROM df4)\n",
    "                WHERE row = 1\n",
    "                ORDER BY chapter ASC\"\"\")\n",
    "spark.sql(query).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
