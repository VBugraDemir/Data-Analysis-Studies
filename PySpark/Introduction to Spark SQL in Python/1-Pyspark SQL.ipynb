{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Window functions perform a calculation across rows that are related to the current row. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and querying a SQL table in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SQL table from a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from urllib.request import urlretrieve\n",
    "# url = \"https://assets.datacamp.com/production/repositories/3937/datasets/a367f6f461f670a364ab2a59afc25bc2e3fab157/trainsched.txt\"\n",
    "# data = urlretrieve(url,\"trainsched.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"pyspark-shell\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----+\n",
      "|train_id|      station|time|\n",
      "+--------+-------------+----+\n",
      "|     324|San Francisco|7:59|\n",
      "|     324|  22nd Street|8:03|\n",
      "|     324|     Millbrae|8:16|\n",
      "|     324|    Hillsdale|8:24|\n",
      "|     324| Redwood City|8:31|\n",
      "|     324|    Palo Alto|8:37|\n",
      "|     324|     San Jose|9:05|\n",
      "|     217|       Gilroy|6:06|\n",
      "|     217|   San Martin|6:15|\n",
      "|     217|  Morgan Hill|6:21|\n",
      "|     217| Blossom Hill|6:36|\n",
      "|     217|      Capitol|6:42|\n",
      "|     217|       Tamien|6:50|\n",
      "|     217|     San Jose|6:59|\n",
      "+--------+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"trainsched.txt\", header=True)\n",
    "df.createOrReplaceTempView(\"table1\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the column names of a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|col_name|\n",
      "+--------+\n",
      "|train_id|\n",
      "| station|\n",
      "|    time|\n",
      "+--------+\n",
      "\n",
      "+--------+-------+----+\n",
      "|train_id|station|time|\n",
      "+--------+-------+----+\n",
      "+--------+-------+----+\n",
      "\n",
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|train_id|   string|   null|\n",
      "| station|   string|   null|\n",
      "|    time|   string|   null|\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW COLUMNS FROM table1\").show()\n",
    "\n",
    "spark.sql(\"SELECT * FROM table1 LIMIT 0\").show()\n",
    "\n",
    "spark.sql(\"DESCRIBE table1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window function SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A window function operates on a set of rows and retuns a value for each row in the set and value can depend on other rows in the set. \n",
    "\n",
    "OVER clause designates this query as a window function query and must contain an ORDER BY clause that tells it how to sequence the rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running sums using window function SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----+---------+\n",
      "|train_id|      station|time|time_next|\n",
      "+--------+-------------+----+---------+\n",
      "|     217|       Gilroy|6:06|     6:15|\n",
      "|     217|   San Martin|6:15|     6:21|\n",
      "|     217|  Morgan Hill|6:21|     6:36|\n",
      "|     217| Blossom Hill|6:36|     6:42|\n",
      "|     217|      Capitol|6:42|     6:50|\n",
      "|     217|       Tamien|6:50|     6:59|\n",
      "|     217|     San Jose|6:59|     null|\n",
      "|     324|San Francisco|7:59|     8:03|\n",
      "|     324|  22nd Street|8:03|     8:16|\n",
      "|     324|     Millbrae|8:16|     8:24|\n",
      "|     324|    Hillsdale|8:24|     8:31|\n",
      "|     324| Redwood City|8:31|     8:37|\n",
      "|     324|    Palo Alto|8:37|     9:05|\n",
      "|     324|     San Jose|9:05|     null|\n",
      "+--------+-------------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"SELECT *, LEAD(time, 1) OVER (PARTITION BY train_id ORDER BY time) AS time_next\n",
    "FROM table1 \"\"\"\n",
    "df =spark.sql(query)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----+---------+-------------------+-------------------+\n",
      "|train_id|      station|time|time_next|     time_timestamp|time_next_timestamp|\n",
      "+--------+-------------+----+---------+-------------------+-------------------+\n",
      "|     217|       Gilroy|6:06|     6:15|2021-09-19 06:06:00|2021-09-19 06:15:00|\n",
      "|     217|   San Martin|6:15|     6:21|2021-09-19 06:15:00|2021-09-19 06:21:00|\n",
      "|     217|  Morgan Hill|6:21|     6:36|2021-09-19 06:21:00|2021-09-19 06:36:00|\n",
      "|     217| Blossom Hill|6:36|     6:42|2021-09-19 06:36:00|2021-09-19 06:42:00|\n",
      "|     217|      Capitol|6:42|     6:50|2021-09-19 06:42:00|2021-09-19 06:50:00|\n",
      "|     217|       Tamien|6:50|     6:59|2021-09-19 06:50:00|2021-09-19 06:59:00|\n",
      "|     217|     San Jose|6:59|     null|2021-09-19 06:59:00|               null|\n",
      "|     324|San Francisco|7:59|     8:03|2021-09-19 07:59:00|2021-09-19 08:03:00|\n",
      "|     324|  22nd Street|8:03|     8:16|2021-09-19 08:03:00|2021-09-19 08:16:00|\n",
      "|     324|     Millbrae|8:16|     8:24|2021-09-19 08:16:00|2021-09-19 08:24:00|\n",
      "|     324|    Hillsdale|8:24|     8:31|2021-09-19 08:24:00|2021-09-19 08:31:00|\n",
      "|     324| Redwood City|8:31|     8:37|2021-09-19 08:31:00|2021-09-19 08:37:00|\n",
      "|     324|    Palo Alto|8:37|     9:05|2021-09-19 08:37:00|2021-09-19 09:05:00|\n",
      "|     324|     San Jose|9:05|     null|2021-09-19 09:05:00|               null|\n",
      "+--------+-------------+----+---------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"time_timestamp\", df.time.cast(\"timestamp\"))\n",
    "\n",
    "df = df.withColumn(\"time_next_timestamp\", df.time_next.cast(\"timestamp\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----+----+\n",
      "|train_id|      station|time|diff|\n",
      "+--------+-------------+----+----+\n",
      "|     217|       Gilroy|6:06| 9.0|\n",
      "|     217|   San Martin|6:15| 6.0|\n",
      "|     217|  Morgan Hill|6:21|15.0|\n",
      "|     217| Blossom Hill|6:36| 6.0|\n",
      "|     217|      Capitol|6:42| 8.0|\n",
      "|     217|       Tamien|6:50| 9.0|\n",
      "|     217|     San Jose|6:59|null|\n",
      "|     324|San Francisco|7:59| 4.0|\n",
      "|     324|  22nd Street|8:03|13.0|\n",
      "|     324|     Millbrae|8:16| 8.0|\n",
      "|     324|    Hillsdale|8:24| 7.0|\n",
      "|     324| Redwood City|8:31| 6.0|\n",
      "|     324|    Palo Alto|8:37|28.0|\n",
      "|     324|     San Jose|9:05|null|\n",
      "+--------+-------------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp\n",
    "df = df.withColumn(\"diff\", (unix_timestamp(df.time_next_timestamp)-unix_timestamp(df.time_timestamp))/60)\n",
    "df.select(\"train_id\", \"station\", \"time\",\"diff\").show()\n",
    "df.createOrReplaceTempView(\"table1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----+----+-------------+\n",
      "|train_id|      station|time|diff|running_total|\n",
      "+--------+-------------+----+----+-------------+\n",
      "|     217|       Gilroy|6:06| 9.0|          9.0|\n",
      "|     217|   San Martin|6:15| 6.0|         15.0|\n",
      "|     217|  Morgan Hill|6:21|15.0|         30.0|\n",
      "|     217| Blossom Hill|6:36| 6.0|         36.0|\n",
      "|     217|      Capitol|6:42| 8.0|         44.0|\n",
      "|     217|       Tamien|6:50| 9.0|         53.0|\n",
      "|     217|     San Jose|6:59|null|         53.0|\n",
      "|     324|San Francisco|7:59| 4.0|          4.0|\n",
      "|     324|  22nd Street|8:03|13.0|         17.0|\n",
      "|     324|     Millbrae|8:16| 8.0|         25.0|\n",
      "|     324|    Hillsdale|8:24| 7.0|         32.0|\n",
      "|     324| Redwood City|8:31| 6.0|         38.0|\n",
      "|     324|    Palo Alto|8:37|28.0|         66.0|\n",
      "|     324|     San Jose|9:05|null|         66.0|\n",
      "+--------+-------------+----+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"SELECT train_id, station, time, diff, \n",
    "SUM(diff) OVER(PARTITION BY train_id ORDER BY time) AS running_total FROM table1\"\"\"\n",
    "\n",
    "spark.sql(query).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot notation and SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is typically a dot notation equivalent of every SQL caluse including window functions.\n",
    "\n",
    "* df.select(\"train_id\", \"station\")\n",
    "* df.select(df.train_id, df.station)\n",
    "* df.select(col(\"train_id\"), col(\"station\"))\n",
    "\n",
    "\n",
    "* df.select(\"train_id\", \"station\").withColumnRenamed(\"train_id\", \"train\") \n",
    "* df.select(col(\"train_id\").alias(\"train\"), \"station\")\n",
    "\n",
    "\n",
    "* spark.sql(\"SELECT train_id AS train, station FROM schedule LIMIT 5\")\n",
    "* df.select(col(\"train_id\").alias(\"train\"), \"station\").limit(5).show()\n",
    "\n",
    "Window functions can also be done in either SQL or dot notation.\n",
    "\n",
    "* spark.sql(\"SELECT *, ROW_NUMBER() OVER(PARTITION BY train_id ORDER BY time) AS id FROM schedule\")\n",
    "* df.withColumn(\"id\", row_number().over(Window.partitionBy(\"train_id\").orderBy(\"time\")))\n",
    "\n",
    "\n",
    "* window = Window.partitionBy(\"train_id\").orderBy(\"time\") \n",
    "    * dfx = df.withColumn(\"next\",lead(\"time\", 1).over(window))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation, step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|train_id|start|\n",
      "+--------+-----+\n",
      "|     217| 6:06|\n",
      "|     324| 7:59|\n",
      "+--------+-----+\n",
      "\n",
      "+--------+-----+\n",
      "|train_id|start|\n",
      "+--------+-----+\n",
      "|     217| 6:06|\n",
      "|     324| 7:59|\n",
      "+--------+-----+\n",
      "\n",
      "+--------+---------+---------+\n",
      "|train_id|min(time)|max(time)|\n",
      "+--------+---------+---------+\n",
      "|     217|     6:06|     6:59|\n",
      "|     324|     7:59|     9:05|\n",
      "+--------+---------+---------+\n",
      "\n",
      "+--------+---------+\n",
      "|train_id|max(time)|\n",
      "+--------+---------+\n",
      "|     217|     6:59|\n",
      "|     324|     9:05|\n",
      "+--------+---------+\n",
      "\n",
      "['train_id', 'max(time)']\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT train_id, MIN(time) AS start FROM table1 GROUP BY train_id\").show()\n",
    "df.groupBy(\"train_id\").agg({\"time\":\"min\"}).withColumnRenamed(\"min(time)\", \"start\").show()\n",
    "\n",
    "spark.sql(\"SELECT train_id, MIN(time), MAX(time) FROM table1 GROUP BY train_id\").show()\n",
    "result = df.groupBy(\"train_id\").agg({\"time\":\"min\",\"time\":\"max\"})\n",
    "result.show()\n",
    "print(result.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating the same column twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----+\n",
      "|train_id|start| end|\n",
      "+--------+-----+----+\n",
      "|     217| 6:06|6:59|\n",
      "|     324| 7:59|9:05|\n",
      "+--------+-----+----+\n",
      "\n",
      "+--------+-----+----+\n",
      "|train_id|start| end|\n",
      "+--------+-----+----+\n",
      "|     217| 6:06|6:59|\n",
      "|     324| 7:59|9:05|\n",
      "+--------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max, col\n",
    "\n",
    "expr = [min(col(\"time\")).alias(\"start\"), max(col(\"time\")).alias(\"end\")]\n",
    "dot_df = df.groupBy(\"train_id\").agg(*expr)\n",
    "dot_df.show()\n",
    "\n",
    "query = \"SELECT train_id, MIN(time) AS start, MAX(time) AS end FROM table1 GROUP BY train_id\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate dot SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----+---------+\n",
      "|train_id|      station|time|time_next|\n",
      "+--------+-------------+----+---------+\n",
      "|     217|       Gilroy|6:06|     6:15|\n",
      "|     217|   San Martin|6:15|     6:21|\n",
      "|     217|  Morgan Hill|6:21|     6:36|\n",
      "|     217| Blossom Hill|6:36|     6:42|\n",
      "|     217|      Capitol|6:42|     6:50|\n",
      "|     217|       Tamien|6:50|     6:59|\n",
      "|     217|     San Jose|6:59|     null|\n",
      "|     324|San Francisco|7:59|     8:03|\n",
      "|     324|  22nd Street|8:03|     8:16|\n",
      "|     324|     Millbrae|8:16|     8:24|\n",
      "|     324|    Hillsdale|8:24|     8:31|\n",
      "|     324| Redwood City|8:31|     8:37|\n",
      "|     324|    Palo Alto|8:37|     9:05|\n",
      "|     324|     San Jose|9:05|     null|\n",
      "+--------+-------------+----+---------+\n",
      "\n",
      "+--------+-------------+----+---------+\n",
      "|train_id|      station|time|time_next|\n",
      "+--------+-------------+----+---------+\n",
      "|     217|       Gilroy|6:06|     6:15|\n",
      "|     217|   San Martin|6:15|     6:21|\n",
      "|     217|  Morgan Hill|6:21|     6:36|\n",
      "|     217| Blossom Hill|6:36|     6:42|\n",
      "|     217|      Capitol|6:42|     6:50|\n",
      "|     217|       Tamien|6:50|     6:59|\n",
      "|     217|     San Jose|6:59|     null|\n",
      "|     324|San Francisco|7:59|     8:03|\n",
      "|     324|  22nd Street|8:03|     8:16|\n",
      "|     324|     Millbrae|8:16|     8:24|\n",
      "|     324|    Hillsdale|8:24|     8:31|\n",
      "|     324| Redwood City|8:31|     8:37|\n",
      "|     324|    Palo Alto|8:37|     9:05|\n",
      "|     324|     San Jose|9:05|     null|\n",
      "+--------+-------------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"trainsched.txt\", header=True)\n",
    "df.createOrReplaceTempView(\"table1\")\n",
    "\n",
    "spark.sql(\"\"\"SELECT *, LEAD(time, 1) OVER(PARTITION BY train_id ORDER BY time) AS time_next FROM table1\"\"\").show()\n",
    "\n",
    "from pyspark.sql.functions import lead\n",
    "from pyspark.sql import Window\n",
    "\n",
    "df.withColumn(\"time_next\", lead(\"time\", 1).over(Window.partitionBy(\"train_id\").orderBy(\"time\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert window function from dot notation to SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----+-------------------+\n",
      "|train_id|      station|time|     time_timestamp|\n",
      "+--------+-------------+----+-------------------+\n",
      "|     324|San Francisco|7:59|2021-09-19 07:59:00|\n",
      "|     324|  22nd Street|8:03|2021-09-19 08:03:00|\n",
      "|     324|     Millbrae|8:16|2021-09-19 08:16:00|\n",
      "|     324|    Hillsdale|8:24|2021-09-19 08:24:00|\n",
      "|     324| Redwood City|8:31|2021-09-19 08:31:00|\n",
      "|     324|    Palo Alto|8:37|2021-09-19 08:37:00|\n",
      "|     324|     San Jose|9:05|2021-09-19 09:05:00|\n",
      "|     217|       Gilroy|6:06|2021-09-19 06:06:00|\n",
      "|     217|   San Martin|6:15|2021-09-19 06:15:00|\n",
      "|     217|  Morgan Hill|6:21|2021-09-19 06:21:00|\n",
      "|     217| Blossom Hill|6:36|2021-09-19 06:36:00|\n",
      "|     217|      Capitol|6:42|2021-09-19 06:42:00|\n",
      "|     217|       Tamien|6:50|2021-09-19 06:50:00|\n",
      "|     217|     San Jose|6:59|2021-09-19 06:59:00|\n",
      "+--------+-------------+----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"time_timestamp\", df.time.cast(\"timestamp\"))\n",
    "df.createOrReplaceTempView(\"table1\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----+-------------------+--------+\n",
      "|train_id|      station|time|     time_timestamp|diff_min|\n",
      "+--------+-------------+----+-------------------+--------+\n",
      "|     217|       Gilroy|6:06|2021-09-19 06:06:00|     9.0|\n",
      "|     217|   San Martin|6:15|2021-09-19 06:15:00|     6.0|\n",
      "|     217|  Morgan Hill|6:21|2021-09-19 06:21:00|    15.0|\n",
      "|     217| Blossom Hill|6:36|2021-09-19 06:36:00|     6.0|\n",
      "|     217|      Capitol|6:42|2021-09-19 06:42:00|     8.0|\n",
      "|     217|       Tamien|6:50|2021-09-19 06:50:00|     9.0|\n",
      "|     217|     San Jose|6:59|2021-09-19 06:59:00|    null|\n",
      "|     324|San Francisco|7:59|2021-09-19 07:59:00|     4.0|\n",
      "|     324|  22nd Street|8:03|2021-09-19 08:03:00|    13.0|\n",
      "|     324|     Millbrae|8:16|2021-09-19 08:16:00|     8.0|\n",
      "|     324|    Hillsdale|8:24|2021-09-19 08:24:00|     7.0|\n",
      "|     324| Redwood City|8:31|2021-09-19 08:31:00|     6.0|\n",
      "|     324|    Palo Alto|8:37|2021-09-19 08:37:00|    28.0|\n",
      "|     324|     San Jose|9:05|2021-09-19 09:05:00|    null|\n",
      "+--------+-------------+----+-------------------+--------+\n",
      "\n",
      "+--------+-------------+----+-------------------+--------+\n",
      "|train_id|      station|time|     time_timestamp|diff_min|\n",
      "+--------+-------------+----+-------------------+--------+\n",
      "|     217|       Gilroy|6:06|2021-09-19 06:06:00|     9.0|\n",
      "|     217|   San Martin|6:15|2021-09-19 06:15:00|     6.0|\n",
      "|     217|  Morgan Hill|6:21|2021-09-19 06:21:00|    15.0|\n",
      "|     217| Blossom Hill|6:36|2021-09-19 06:36:00|     6.0|\n",
      "|     217|      Capitol|6:42|2021-09-19 06:42:00|     8.0|\n",
      "|     217|       Tamien|6:50|2021-09-19 06:50:00|     9.0|\n",
      "|     217|     San Jose|6:59|2021-09-19 06:59:00|    null|\n",
      "|     324|San Francisco|7:59|2021-09-19 07:59:00|     4.0|\n",
      "|     324|  22nd Street|8:03|2021-09-19 08:03:00|    13.0|\n",
      "|     324|     Millbrae|8:16|2021-09-19 08:16:00|     8.0|\n",
      "|     324|    Hillsdale|8:24|2021-09-19 08:24:00|     7.0|\n",
      "|     324| Redwood City|8:31|2021-09-19 08:31:00|     6.0|\n",
      "|     324|    Palo Alto|8:37|2021-09-19 08:37:00|    28.0|\n",
      "|     324|     San Jose|9:05|2021-09-19 09:05:00|    null|\n",
      "+--------+-------------+----+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window = Window.partitionBy('train_id').orderBy('time')\n",
    "dot_df = df.withColumn('diff_min', \n",
    "                    (unix_timestamp(lead('time', 1).over(window),'H:m') \n",
    "                     - unix_timestamp('time', 'H:m'))/60)\n",
    "dot_df.show()\n",
    "\n",
    "query = \"\"\"SELECT *, (UNIX_TIMESTAMP(LEAD(time_timestamp, 1) OVER (PARTITION BY train_id ORDER BY time_timestamp), 'H:m') \n",
    "- UNIX_TIMESTAMP(time_timestamp, 'H:m'))/60 AS diff_min FROM table1\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
