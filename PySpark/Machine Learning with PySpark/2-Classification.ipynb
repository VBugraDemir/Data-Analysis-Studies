{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"pyspark-shell\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the missing values are not that high in amount, you can simply remove the record with the missing value. You can use the filter method or more aggresively you can drop all missing values in any column. This should be donw with care because it could result in the loss of a lot of otherwise useful data. \n",
    "\n",
    "String columns that represent categories should be converted to integer by using StringIndexer. Fit and transform the data using the indexer. Index values are assigned according to the descending relative frequency of each of the string values. Rather than using frequency of occurence, string can be ordered alpabetically (stringOrderType).\n",
    "\n",
    "The final step in preparing the data is to consolidate the varius input columns into a single column. Machine learning algorithms in Spark operate on a single vector of predictors (VectorAssembler)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing columns and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2978\n",
      "47022\n"
     ]
    }
   ],
   "source": [
    "flights = spark.read.csv(\"flights.csv\", sep=\",\", header=True, inferSchema=True, nullValue=\"NA\")\n",
    "\n",
    "flights_drop_column = flights.drop(\"flight\")\n",
    "print(flights_drop_column.filter(\"delay IS NULL\").count())\n",
    "flights_valid_delay = flights_drop_column.filter(\"delay IS NOT NULL\")\n",
    "flights_none_missing = flights_valid_delay.dropna()\n",
    "print(flights_none_missing.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "|  0| 22|  2|     UA|ORD| 16.33|      82|   30| 509.0|    1|\n",
      "|  2| 20|  4|     UA|SFO|  6.17|      82|   -8| 542.0|    0|\n",
      "|  9| 13|  1|     AA|ORD| 10.33|     195|   -5|1989.0|    0|\n",
      "|  5|  2|  1|     UA|SFO|  7.98|     102|    2| 885.0|    0|\n",
      "|  7|  2|  6|     AA|ORD| 10.83|     135|   54|1180.0|    1|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "flights_km = flights_none_missing.withColumn(\"km\", round(flights_none_missing.mile * 1.60934, 0)).drop(\"mile\")\n",
    "flights_km = flights_km.withColumn(\"label\", (flights_none_missing.delay >=15).cast(\"integer\"))\n",
    "flights_km.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical columns (org and carrier) will be transformed into indexed numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|carrier_idx|org_idx|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "|  0| 22|  2|     UA|ORD| 16.33|      82|   30| 509.0|    1|        0.0|    0.0|\n",
      "|  2| 20|  4|     UA|SFO|  6.17|      82|   -8| 542.0|    0|        0.0|    1.0|\n",
      "|  9| 13|  1|     AA|ORD| 10.33|     195|   -5|1989.0|    0|        1.0|    0.0|\n",
      "|  5|  2|  1|     UA|SFO|  7.98|     102|    2| 885.0|    0|        0.0|    1.0|\n",
      "|  7|  2|  6|     AA|ORD| 10.83|     135|   54|1180.0|    1|        1.0|    0.0|\n",
      "|  1| 16|  6|     UA|ORD|   8.0|     232|   -7|2317.0|    0|        0.0|    0.0|\n",
      "|  1| 22|  5|     UA|SJC|  7.98|     250|  -13|2943.0|    0|        0.0|    5.0|\n",
      "| 11|  8|  1|     OO|SFO|  7.77|      60|   88| 254.0|    1|        2.0|    1.0|\n",
      "|  4| 26|  1|     AA|SFO| 13.25|     210|  -10|2356.0|    0|        1.0|    1.0|\n",
      "|  4| 25|  0|     AA|ORD| 13.75|     160|   31|1574.0|    1|        1.0|    0.0|\n",
      "|  8| 30|  2|     UA|ORD| 13.28|     151|   16|1157.0|    1|        0.0|    0.0|\n",
      "|  3| 16|  3|     UA|ORD|   9.0|     264|    3|2808.0|    0|        0.0|    0.0|\n",
      "|  0|  3|  4|     AA|LGA| 17.08|     190|   32|1765.0|    1|        1.0|    3.0|\n",
      "|  5|  9|  1|     UA|SFO|  12.7|     158|   20|1556.0|    1|        0.0|    1.0|\n",
      "|  3| 10|  4|     B6|ORD| 17.58|     265|  155|2792.0|    1|        4.0|    0.0|\n",
      "| 11| 15|  1|     AA|ORD|  6.75|     160|   23|1291.0|    1|        1.0|    0.0|\n",
      "|  8| 18|  4|     UA|SJC|  6.33|     160|   17|1526.0|    1|        0.0|    5.0|\n",
      "|  2| 14|  5|     B6|JFK|  6.17|     166|    0|1519.0|    0|        4.0|    2.0|\n",
      "|  7| 21|  4|     OO|ORD|  19.0|     110|   21| 977.0|    1|        2.0|    0.0|\n",
      "| 11|  6|  6|     OO|SFO|  8.75|      82|   40| 509.0|    1|        2.0|    1.0|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"carrier\", outputCol=\"carrier_idx\")\n",
    "indexer_model = indexer.fit(flights_km)\n",
    "flights_indexed = indexer_model.transform(flights_km)\n",
    "flights_indexed = StringIndexer(inputCol=\"org\", outputCol=\"org_idx\").fit(flights_indexed).transform(flights_indexed)\n",
    "flights_indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+-----+\n",
      "|features                                 |delay|\n",
      "+-----------------------------------------+-----+\n",
      "|[0.0,22.0,2.0,0.0,0.0,509.0,16.33,82.0]  |30   |\n",
      "|[2.0,20.0,4.0,0.0,1.0,542.0,6.17,82.0]   |-8   |\n",
      "|[9.0,13.0,1.0,1.0,0.0,1989.0,10.33,195.0]|-5   |\n",
      "|[5.0,2.0,1.0,0.0,1.0,885.0,7.98,102.0]   |2    |\n",
      "|[7.0,2.0,6.0,1.0,0.0,1180.0,10.83,135.0] |54   |\n",
      "+-----------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols= [\"mon\", \"dom\", \"dow\", \"carrier_idx\", \"org_idx\", \"km\", \"depart\", \"duration\"],\n",
    "                            outputCol=\"features\")\n",
    "fligt_assembled = assembler.transform(flights_indexed)\n",
    "fligt_assembled.select(\"features\", \"delay\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision Tree is constructed using an algorithm called \"Recursive  Partitionong\". \n",
    "\n",
    "Random split method to randomly split data into two sets, a training set and a testing set.\n",
    "\n",
    "Accuracy = (TN + TP) / (TN + TP + FN + FP) proportion of correct predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9253168534618204\n"
     ]
    }
   ],
   "source": [
    "flights_train, flights_test = fligt_assembled.randomSplit([0.8, 0.2], 17)\n",
    "training_ratio = flights_train.count() / flights_test.count()\n",
    "print(training_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------------------------------------+\n",
      "|label|prediction|probability                             |\n",
      "+-----+----------+----------------------------------------+\n",
      "|1    |0.0       |[0.5386092969650403,0.46139070303495966]|\n",
      "|0    |1.0       |[0.3535976324064369,0.6464023675935631] |\n",
      "|0    |0.0       |[0.5386092969650403,0.46139070303495966]|\n",
      "|1    |1.0       |[0.3535976324064369,0.6464023675935631] |\n",
      "|1    |1.0       |[0.3535976324064369,0.6464023675935631] |\n",
      "+-----+----------+----------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree_model = tree.fit(flights_train)\n",
    "prediction = tree_model.transform(flights_test)\n",
    "prediction.select(\"label\", \"prediction\", \"probability\").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0| 1725|\n",
      "|    0|       0.0| 2828|\n",
      "|    1|       1.0| 3185|\n",
      "|    0|       1.0| 1809|\n",
      "+-----+----------+-----+\n",
      "\n",
      "0.6298313606368493\n"
     ]
    }
   ],
   "source": [
    "prediction.groupBy(\"label\", 'prediction').count().show()\n",
    "\n",
    "TN = prediction.filter(\"label = 0 AND prediction = label\").count()\n",
    "TP = prediction.filter(\"label = 1 AND prediction = label\").count()\n",
    "FP = prediction.filter(\"label = 1 AND prediction != label\").count()\n",
    "FN = prediction.filter(\"label = 0 AND prediction != label\").count()\n",
    "\n",
    "accuracy = (TN + TP) / (TN + TP + FP + FN)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's another commonly used classification model. It uses a logistic function to model a binary target (1,0 or TRUE, FALSE). \n",
    "\n",
    "Coefficients can shift the curve to the right or to the left. They might make the transistion between states more gradual or more rapid. These characteristics are all extracted from the training data and will vary from one set of data to another. \n",
    "\n",
    "Precision is the proportion of positive predictions which are correct. TP / (TP + FP)\n",
    "\n",
    "Recall is the proportion of positive targets which are correctly predicted. TP / (TP + FN)\n",
    "\n",
    "Choosing a larger or smaller value for the threshold will affect the performance of the model. The ROC curve plots the true positive rate versus the false positive rate. AUC is the are under the ROC curve. AUC indicates how well a model performs across all values of the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0| 6707|\n",
      "|    0|       0.0|10445|\n",
      "|    1|       1.0|12426|\n",
      "|    0|       1.0| 7897|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "flights_train_lr = flights_train.select(\"mon\", \"depart\", \"duration\", \"features\", \"label\")\n",
    "flights_test_lr = flights_test.select(\"mon\", \"depart\", \"duration\", \"features\", \"label\")\n",
    "\n",
    "logistic = LogisticRegression().fit(flights_train_lr)\n",
    "prediction = logistic.transform(flights_train_lr)\n",
    "\n",
    "prediction.groupBy(\"label\", \"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision = 0.65\n",
      "recall    = 0.61\n",
      "0.6102221286967238\n",
      "0.6480160400989486\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "TN = prediction.filter(\"label = 0 AND prediction = label\").count()\n",
    "TP = prediction.filter(\"label = 1 AND prediction = label\").count()\n",
    "FP = prediction.filter(\"label = 1 AND prediction != label\").count()\n",
    "FN = prediction.filter(\"label = 0 AND prediction != label\").count()\n",
    "\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "print('precision = {:.2f}\\nrecall    = {:.2f}'.format(precision, recall))\n",
    "\n",
    "multi_evaluator = MulticlassClassificationEvaluator()\n",
    "weighted_precision = multi_evaluator.evaluate(prediction, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
    "print(weighted_precision)\n",
    "\n",
    "binary_evaluator = BinaryClassificationEvaluator()\n",
    "auc = binary_evaluator.evaluate(prediction, {binary_evaluator.metricName:\"areaUnderROC\"})\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weighted precision indicates what proportion of predictions (positive and negative) are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning Text into Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing. You split the text into words or tokens. Each document will be transformed into a list of words. Stop words will be removed using StopWordsRemover. It would also be handy to convert the words into numbers. Hashing trick is converting words into numbers. The output in the hash column is presented in sparse format. The first list contains the hashed values and the second list indicates how many time each of those values occurs.\n",
    "\n",
    "If a word appears in many documents the it's probably going to be less useful for building a classifier. Weight the number of counts for a word in a particular document against how frequently that word occurs across all documents. The effective count are reduced for more common words, giving what is known as the \"inverse document frequency\"\n",
    "\n",
    "* remove punctuation and numbers\n",
    "* tokenize (split into individual words)\n",
    "* remove stop words\n",
    "* apply the hashing trick\n",
    "* convert to TF-IDF representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation, numbers and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------+-----+------------------------------------------+\n",
      "|id |text                              |label|words                                     |\n",
      "+---+----------------------------------+-----+------------------------------------------+\n",
      "|1  |Sorry i'll call later in meeting  |0    |[sorry, i'll, call, later, in, meeting]   |\n",
      "|2  |Dont worry i guess he's busy      |0    |[dont, worry, i, guess, he's, busy]       |\n",
      "|3  |Call FREEPHONE now                |1    |[call, freephone, now]                    |\n",
      "|4  |Win a cash prize or a prize worth |1    |[win, a, cash, prize, or, a, prize, worth]|\n",
      "+---+----------------------------------+-----+------------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sms = spark.read.csv(\"sms.csv\", sep=\";\",header=True, inferSchema=True)\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "wrangled = sms.withColumn(\"text\", regexp_replace(sms.text, '[_():;,.!?\\\\-]', \" \"))\n",
    "wrangled = wrangled.withColumn(\"text\", regexp_replace(wrangled.text, \"[0-9]\", \" \"))\n",
    "wrangled = wrangled.withColumn(\"text\", regexp_replace(wrangled.text, \" +\", \" \"))\n",
    "wrangled = wrangled.withColumn(\"text\", regexp_replace(wrangled.text, \"I\", \"i\"))\n",
    "wrangled = Tokenizer(inputCol=\"text\", outputCol=\"words\").transform(wrangled)\n",
    "wrangled.show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words and hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDF calculation weight of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|terms                           |features                                                                                            |\n",
      "+--------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|[sorry, call, later, meeting]   |(1024,[138,384,577,996],[2.273418200008753,3.6288353225642043,3.5890949939146903,4.104259019279279])|\n",
      "|[dont, worry, guess, busy]      |(1024,[215,233,276,329],[3.9913186080986836,3.3790235241678332,4.734227298217693,4.58299632849377]) |\n",
      "|[call, freephone]               |(1024,[133,138],[5.367951058306837,2.273418200008753])                                              |\n",
      "|[win, cash, prize, prize, worth]|(1024,[31,47,62,389],[3.6632029660684124,4.754846585420428,4.072170704727778,7.064594791043114])    |\n",
      "+--------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover, HashingTF, IDF\n",
    "\n",
    "sw_removed = StopWordsRemover(inputCol=\"words\", outputCol=\"terms\").transform(wrangled)\n",
    "hashed = HashingTF(inputCol=\"terms\", outputCol=\"hash\", numFeatures=1024).transform(sw_removed)\n",
    "tf_idf = IDF(inputCol=\"hash\", outputCol=\"features\").fit(hashed).transform(hashed)\n",
    "\n",
    "tf_idf.select('terms', 'features').show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a spam classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0|   41|\n",
      "|    0|       0.0|  948|\n",
      "|    1|       1.0|  105|\n",
      "|    0|       1.0|    2|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sms_train, sms_test = tf_idf.randomSplit([0.8, 0.2], 13)\n",
    "\n",
    "logistic = LogisticRegression(regParam=0.2).fit(sms_train)\n",
    "prediction = logistic.transform(sms_test)\n",
    "prediction.groupBy(\"label\", \"prediction\").count().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
