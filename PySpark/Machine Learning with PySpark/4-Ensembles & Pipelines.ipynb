{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"pyspark-shell\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembles & Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines will seriously steamline your workflow. They will also help to ensure that training and testing data are treated consistently and that no leakage of information between these two sets takes place.\n",
    "\n",
    "A pipeline is a mechanism to combine series of steps rather than applying each of the steps individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flight duration model: Pipeline stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "flights = spark.read.csv(\"flights.csv\", sep=\",\", header=True, inferSchema=True, nullValue=\"NA\")\n",
    "flights = flights.withColumn(\"km\", round(flights.mile * 1.60934, 0)).drop(\"mile\")\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"org\", outputCol=\"org_idx\")\n",
    "onehot = OneHotEncoder(inputCols=[\"org_idx\", \"dow\"], outputCols=[\"org_dummy\", \"dow_dummy\"])\n",
    "assembler = VectorAssembler(inputCols=[\"km\", \"org_dummy\", \"dow_dummy\"], outputCol=\"features\")\n",
    "regression = LinearRegression(labelCol=\"duration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flight duration model: Pipeline model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_train, flights_test = flights.randomSplit([0.8,0.2], 13)\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[indexer, onehot, assembler, regression])\n",
    "pipeline = pipeline.fit(flights_train)\n",
    "predictions = pipeline.transform(flights_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+------------------+\n",
      "|features                      |duration|prediction        |\n",
      "+------------------------------+--------+------------------+\n",
      "|(14,[0,3,10],[4162.0,1.0,1.0])|385     |377.34661750108603|\n",
      "|(14,[0,3,10],[3983.0,1.0,1.0])|379     |364.04381688061255|\n",
      "|(14,[0,1,10],[1180.0,1.0,1.0])|130     |131.59537849620068|\n",
      "|(14,[0,3,10],[2570.0,1.0,1.0])|230     |259.0334410329645 |\n",
      "|(14,[0,1,10],[378.0,1.0,1.0]) |64      |71.99288633072742 |\n",
      "+------------------------------+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"features\", \"duration\", \"prediction\").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMS spam pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"terms\")\n",
    "hasher = HashingTF(inputCol=\"terms\", outputCol=\"hash\")\n",
    "idf = IDF(inputCol=\"hash\", outputCol=\"features\")\n",
    "logistic = LogisticRegression()\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hasher, idf, logistic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "sms = spark.read.csv(\"sms.csv\", sep=\";\",header=True, inferSchema=True)\n",
    "sms = sms.withColumn(\"text\", regexp_replace(sms.text, '[_():;,.!?\\\\-]', \" \"))\n",
    "sms = sms.withColumn(\"text\", regexp_replace(sms.text, \"[0-9]\", \" \"))\n",
    "sms = sms.withColumn(\"text\", regexp_replace(sms.text, \" +\", \" \"))\n",
    "sms = sms.withColumn(\"text\", regexp_replace(sms.text, \"I\", \"i\"))\n",
    "\n",
    "sms_train, sms_test = sms.randomSplit([0.8, 0.2], 13)\n",
    "\n",
    "pipeline = pipeline.fit(sms_train)\n",
    "predictions = pipeline.transform(sms_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|                text|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|Dont worry i gues...|    0|       0.0|\n",
      "|Ok lar Joking wif...|    0|       0.0|\n",
      "|WiNNER As a value...|    1|       1.0|\n",
      "|England v Macedon...|    1|       0.0|\n",
      "|is that seriously...|    0|       0.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"text\", \"label\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1837365616517062"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator()\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple training and test data  approach has one major drawback which is only getting one estimate of the model performance. You would have more robust idea of how well a model works if you were able to test it multiple times. This is the ideo behind cross-validation. Splitting **training data** into folds, selecting one of them and using as training set the rest, repeat the process for number of fold times. Then you can calculate the average of the evaluation metric over all folds, which is a much more ronust measure of model performance than a single value.\n",
    "\n",
    "You need CrossValidator and ParamGridBuilder from pyspark.ml.tuning for this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validating simple flight duration model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation provides a much better way to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------+------------------+\n",
      "|   km|duration|features|        prediction|\n",
      "+-----+--------+--------+------------------+\n",
      "|108.0|      43| [108.0]|52.547663297131706|\n",
      "|108.0|      44| [108.0]|52.547663297131706|\n",
      "|108.0|      46| [108.0]|52.547663297131706|\n",
      "|108.0|      46| [108.0]|52.547663297131706|\n",
      "|108.0|      46| [108.0]|52.547663297131706|\n",
      "|108.0|      47| [108.0]|52.547663297131706|\n",
      "|108.0|      47| [108.0]|52.547663297131706|\n",
      "|108.0|      47| [108.0]|52.547663297131706|\n",
      "|108.0|      47| [108.0]|52.547663297131706|\n",
      "|108.0|      48| [108.0]|52.547663297131706|\n",
      "|108.0|      49| [108.0]|52.547663297131706|\n",
      "|108.0|      49| [108.0]|52.547663297131706|\n",
      "|108.0|      49| [108.0]|52.547663297131706|\n",
      "|108.0|      49| [108.0]|52.547663297131706|\n",
      "|108.0|      51| [108.0]|52.547663297131706|\n",
      "|108.0|      53| [108.0]|52.547663297131706|\n",
      "|108.0|      53| [108.0]|52.547663297131706|\n",
      "|124.0|      39| [124.0]| 53.75725524710285|\n",
      "|124.0|      39| [124.0]| 53.75725524710285|\n",
      "|124.0|      40| [124.0]| 53.75725524710285|\n",
      "+-----+--------+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_km = flights.select(\"km\", \"duration\")\n",
    "\n",
    "assembler= VectorAssembler(inputCols=[\"km\"], outputCol=\"features\")\n",
    "flights_km = assembler.transform(flights_km)\n",
    "\n",
    "flights_train, flights_test = flights_km.randomSplit([0.8,0.2],13)\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "\n",
    "params = ParamGridBuilder().build()\n",
    "regression = LinearRegression(labelCol=\"duration\")\n",
    "evaluator = RegressionEvaluator(labelCol=\"duration\")\n",
    "\n",
    "cv = CrossValidator(estimator=regression, estimatorParamMaps=params, evaluator=evaluator, numFolds=5)\n",
    "cv = cv.fit(flights_train)\n",
    "\n",
    "prediction = cv.transform(flights_test)\n",
    "prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17.027105424959803]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.518010870137793"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(cv.transform(flights_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validating flight duration model pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.271797559154411]\n",
      "11.012777355873434\n"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol=\"org\", outputCol=\"org_idx\")\n",
    "onehot = OneHotEncoder(inputCols=[\"org_idx\"], outputCols=[\"org_dummy\"])\n",
    "assembler = VectorAssembler(inputCols=[\"km\", \"org_dummy\"], outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[indexer, onehot, assembler, regression])\n",
    "cv = CrossValidator(estimator=pipeline,\n",
    "                   estimatorParamMaps=params,\n",
    "                   evaluator=evaluator)\n",
    "\n",
    "flights_train, flights_test = flights.randomSplit([0.8, 0.2], 13)\n",
    "\n",
    "cv = cv.fit(flights_train)\n",
    "print(cv.avgMetrics)\n",
    "\n",
    "prediction = cv.transform(flights_test)\n",
    "print(evaluator.evaluate(cv.transform(flights_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models can be improved by choosing better model parameters. The optimal choice of paramaters will depend on the data and the modeling goal. For example setting fitIntercept as True or False. It'd be better to make this comparison using cross-validation and automated. \n",
    "\n",
    "You can systematically evaluate a model across a grid of parameter values using a technique known as grid search. First you create a grid builder and then you add one or more grids such as fitIntercept. Call the build() method to construct the grid. After creating a cross-validator object and fit it to the training data you can retrieve the best model using the bestModel attribut of cv. But it's not necessary since the cv object will behave like the best model. So you can use it directly to make predictions on the testing data. You can retrieve the best parameter value by using explainParam method.\n",
    "\n",
    "The more parameter and values you add to the grid, the more models you have to evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing flights linear regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models to be tested:  12\n"
     ]
    }
   ],
   "source": [
    "params = ParamGridBuilder()\n",
    "params = params.addGrid(regression.regParam,[0.01, 0.1, 1, 10]).addGrid(regression.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "params = params.build()\n",
    "print(\"Number of models to be tested: \", len(params))\n",
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=params, evaluator=evaluator, numFolds=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dissecting the best flight duration model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StringIndexerModel: uid=StringIndexer_8da5c08a38e3, handleInvalid=error, OneHotEncoderModel: uid=OneHotEncoder_cb76bce9219f, dropLast=true, handleInvalid=error, numInputCols=1, numOutputCols=1, VectorAssembler_29d0c071636a, LinearRegressionModel: uid=LinearRegression_665672928f84, numFeatures=8]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11.012865738560498"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = cv.fit(flights_train)\n",
    "\n",
    "best_model = cv.bestModel\n",
    "print(best_model.stages)\n",
    "\n",
    "best_model.stages[3].extractParamMap()\n",
    "\n",
    "predictions = best_model.transform(flights_test)\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LinearRegression_665672928f84', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2,\n",
       " Param(parent='LinearRegression_665672928f84', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0,\n",
       " Param(parent='LinearRegression_665672928f84', name='epsilon', doc='The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber'): 1.35,\n",
       " Param(parent='LinearRegression_665672928f84', name='featuresCol', doc='features column name.'): 'features',\n",
       " Param(parent='LinearRegression_665672928f84', name='fitIntercept', doc='whether to fit an intercept term.'): True,\n",
       " Param(parent='LinearRegression_665672928f84', name='labelCol', doc='label column name.'): 'duration',\n",
       " Param(parent='LinearRegression_665672928f84', name='loss', doc='The loss function to be optimized. Supported options: squaredError, huber.'): 'squaredError',\n",
       " Param(parent='LinearRegression_665672928f84', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'): 0.0,\n",
       " Param(parent='LinearRegression_665672928f84', name='maxIter', doc='max number of iterations (>= 0).'): 100,\n",
       " Param(parent='LinearRegression_665672928f84', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
       " Param(parent='LinearRegression_665672928f84', name='regParam', doc='regularization parameter (>= 0).'): 0.01,\n",
       " Param(parent='LinearRegression_665672928f84', name='solver', doc='The solver algorithm for optimization. Supported options: auto, normal, l-bfgs.'): 'auto',\n",
       " Param(parent='LinearRegression_665672928f84', name='standardization', doc='whether to standardize the training features before fitting the model.'): True,\n",
       " Param(parent='LinearRegression_665672928f84', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.stages[3].extractParamMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMS spam optimised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ParamGridBuilder()\n",
    "params = params.addGrid(hasher.numFeatures, [1024, 4096,16384]).addGrid(hasher.binary, [True, False])\n",
    "params = params.addGrid(logistic.regParam, [0.01, 0.1, 1.0, 10.0,]).addGrid(logistic.elasticNetParam, [0.0, 0.5, 1.0 ])\n",
    "params = params.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer, remover, hasher, idf, logistic])\n",
    "evaluator = RegressionEvaluator()\n",
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=params, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "cv = cv.fit(sms_train)\n",
    "predictions = cv.transform(sms_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1654456490020896\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.evaluate(cv.transform(sms_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='HashingTF_15beaa28d9d2', name='binary', doc='If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts. Default False.'): True,\n",
       " Param(parent='HashingTF_15beaa28d9d2', name='numFeatures', doc='Number of features. Should be greater than 0.'): 4096,\n",
       " Param(parent='HashingTF_15beaa28d9d2', name='outputCol', doc='output column name.'): 'hash',\n",
       " Param(parent='HashingTF_15beaa28d9d2', name='inputCol', doc='input column name.'): 'terms'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.bestModel.stages[2].extractParamMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models can be combined to form a collections or \"ensemble\" which is more powerful tahn each of the individual models alone. An ensemble combines the results from multiple models to produce better predicions than any of those models acting alone. A successful ensemble requires diverse models. Ideally each of the models in the ensemble should be different.\n",
    "\n",
    "A Random Forest is a collections of trees. Each tree trained on random subset of data within each tree a random subset of features is used for splitting at each node. \n",
    "\n",
    "You can make predictions useing each tree individually. In some cases all of the trees agree, but there is often some dissent amongst the models. Transform() method will automatically generate a consensus prediction column. You can see importance of features by using the **featureImportances** attribute. \n",
    "\n",
    "Gradient-Boosted Trees is another ensemble model. Rather tahn building a set of trees that operate in parallel, now we build trees which work in series. The boosting algorithm works iteratively and focuses on improving the incorrect predictions by trying another tree. As trees are added to the ensemble its predictions improve.\n",
    "\n",
    "AUC scores of **Random Forest** and **Greadient-Boosterd Tree** should be better than plain **Decision Tree**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delayed flights with Gradient-Boosted Trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------+-----------------+-----+\n",
      "|mon|depart|duration|         features|label|\n",
      "+---+------+--------+-----------------+-----+\n",
      "|  0| 16.33|      82| [0.0,16.33,82.0]|    1|\n",
      "|  2|  6.17|      82|  [2.0,6.17,82.0]|    0|\n",
      "|  9| 10.33|     195|[9.0,10.33,195.0]|    0|\n",
      "|  5|  7.98|     102| [5.0,7.98,102.0]|    0|\n",
      "|  7| 10.83|     135|[7.0,10.83,135.0]|    1|\n",
      "|  1|   8.0|     232|  [1.0,8.0,232.0]|    0|\n",
      "|  1|  7.98|     250| [1.0,7.98,250.0]|    0|\n",
      "| 11|  7.77|      60| [11.0,7.77,60.0]|    1|\n",
      "|  4| 13.25|     210|[4.0,13.25,210.0]|    0|\n",
      "|  4| 13.75|     160|[4.0,13.75,160.0]|    1|\n",
      "|  8| 13.28|     151|[8.0,13.28,151.0]|    1|\n",
      "|  3|   9.0|     264|  [3.0,9.0,264.0]|    0|\n",
      "|  0| 17.08|     190|[0.0,17.08,190.0]|    1|\n",
      "|  5|  12.7|     158| [5.0,12.7,158.0]|    1|\n",
      "|  3| 17.58|     265|[3.0,17.58,265.0]|    1|\n",
      "| 11|  6.75|     160|[11.0,6.75,160.0]|    1|\n",
      "|  8|  6.33|     160| [8.0,6.33,160.0]|    1|\n",
      "|  2|  6.17|     166| [2.0,6.17,166.0]|    0|\n",
      "|  7|  19.0|     110| [7.0,19.0,110.0]|    1|\n",
      "| 11|  8.75|      82| [11.0,8.75,82.0]|    1|\n",
      "+---+------+--------+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights = flights.dropna(subset=[\"delay\"])\n",
    "flights = flights.withColumn(\"label\", (flights.delay >=15).cast(\"integer\"))\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"mon\", \"depart\", \"duration\"], outputCol=\"features\")\n",
    "\n",
    "flights_labeled = assembler.transform(flights)\n",
    "flights_labeled.select(\"mon\", \"depart\", \"duration\", \"features\", \"label\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6272166509112471\n",
      "0.6801100665589925\n",
      "[DecisionTreeRegressionModel: uid=dtr_b4b18500a002, depth=5, numNodes=63, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_564ab70ab8ff, depth=5, numNodes=63, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_d9929412c476, depth=5, numNodes=63, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_42808eda1715, depth=5, numNodes=63, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_302f97a7c249, depth=5, numNodes=63, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_4281b68c194f, depth=5, numNodes=63, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_24f87a113105, depth=5, numNodes=63, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_f64588ff4af8, depth=5, numNodes=63, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_c262e3d031c6, depth=5, numNodes=63, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_eead2207cd78, depth=5, numNodes=63, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_c6a365fdae3d, depth=5, numNodes=63, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_5b1ea6037878, depth=5, numNodes=63, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_bcc86d9b71f6, depth=5, numNodes=63, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_42249ca4da3a, depth=5, numNodes=61, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_0afcd686f02c, depth=5, numNodes=63, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_540087eb87e5, depth=5, numNodes=63, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_70244d3bdd13, depth=5, numNodes=63, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_9800d1fabca4, depth=5, numNodes=61, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_c0bb910d8e38, depth=5, numNodes=63, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_3a228794e1be, depth=5, numNodes=63, numFeatures=3]\n",
      "\n",
      "(3,[0,1,2],[0.33355841561818106,0.3533380578195992,0.3131035265622197])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "flights_train, flights_test = flights_labeled.randomSplit([0.8, 0.2], 13)\n",
    "\n",
    "tree = DecisionTreeClassifier().fit(flights_train)\n",
    "gbt = GBTClassifier().fit(flights_train)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(evaluator.evaluate(tree.transform(flights_test)))\n",
    "print(evaluator.evaluate(gbt.transform(flights_test)))\n",
    "\n",
    "print(gbt.trees)\n",
    "print()\n",
    "print(gbt.featureImportances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delayed flights with a Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier()\n",
    "\n",
    "params = ParamGridBuilder().addGrid(forest.featureSubsetStrategy, ['all', 'onethird', 'sqrt', 'log2']).addGrid(forest.maxDepth, [2, 5, 10]).build()\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "cv = CrossValidator(estimator=forets, estimatorParamMaps=params, evaluator=evaluator, numFolds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6638777431038031, 0.6638777431038031, 0.6638777431038031, 0.6638777431038031, 0.6638777431038031, 0.6638777431038031, 0.6638777431038031, 0.6638777431038031, 0.6638777431038031, 0.6638777431038031, 0.6638777431038031, 0.6638777431038031] \n",
      "\n",
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5) \n",
      "\n",
      "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto' (default: auto) \n",
      "\n",
      "0.6666110163587886 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = cv.fit(flights_train)\n",
    "\n",
    "print(cv.avgMetrics,\"\\n\")\n",
    "\n",
    "print(cv.bestModel.explainParam(\"maxDepth\"),\"\\n\")\n",
    "print(cv.bestModel.explainParam(\"featureSubsetStrategy\"),\"\\n\")\n",
    "\n",
    "print(evaluator.evaluate(cv.transform(flights_test)),\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
