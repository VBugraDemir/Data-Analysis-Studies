{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most scikit-learn objects are either transformers or models.\n",
    "\n",
    "Transformers are for pre-processing before modeling. The Imputer class (like SimpleImputer for filling in missing values) and FeatureSelection classes in sklearn are an example of some transformers.\n",
    "\n",
    "Models are used to make predictions like Linear Regression model, Decision Tree model, Random Forest model etc. You will usually pre-process your data (with transformers) before putting it in a model.\n",
    "\n",
    "Now the usage of methods fit(), transform(), fit_transform() and predict() depend on the type of object.\n",
    "\n",
    "For Transformers:\n",
    "\n",
    "* fit() - It is used for calculating the initial filling of parameters on the training data (like mean of the column values) and saves them as an internal objects state\n",
    "* transform() - Use the above calculated values and return modified training data\n",
    "* fit_transform() - It joins above two steps. Internally, it just calls first fit() and then transform() on the same data.\n",
    "\n",
    "For Models:\n",
    "\n",
    "* fit() - It calculates the parameters/weights on training data (e.g. parameters returned by coef() in case of Linear Regression) and saves them as an internal objects state.\n",
    "* predict() - Use the above calculated weights on test data to make the predictions\n",
    "* transform() - Cannot be used\n",
    "* fit_transform() - Cannot be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines, feature & text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputer will fill the missing values with the mean of the column in question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     numeric     text  with_missing label\n",
      "0 -10.856306               4.433240     b\n",
      "1   9.973454      foo      4.310229     b\n",
      "2   2.829785  foo bar      2.469828     a\n",
      "3 -15.062947               2.852981     b\n",
      "4  -5.786003  foo bar      1.826475     a\n",
      "\n",
      "Accuracy on sample data - numeric, no nans:  0.62\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "rng = np.random.RandomState(123)\n",
    "\n",
    "SIZE = 1000\n",
    "\n",
    "sample_data = {\n",
    " 'numeric': rng.normal(0, 10, size=SIZE),\n",
    " 'text': rng.choice(['', 'foo', 'bar', 'foo bar', 'bar foo'], size=SIZE),\n",
    " 'with_missing': rng.normal(loc=3, size=SIZE)\n",
    "}\n",
    "\n",
    "sample_df = pd.DataFrame(sample_data)\n",
    "\n",
    "sample_df.loc[rng.choice(sample_df.index, size=np.floor_divide(sample_df.shape[0], 5)), 'with_missing'] = np.nan\n",
    "\n",
    "foo_values = sample_df.text.str.contains('foo') * 10\n",
    "bar_values = sample_df.text.str.contains('bar') * -25\n",
    "no_text = ((foo_values + bar_values) == 0) * 1\n",
    "\n",
    "val = 2 * sample_df.numeric + -2 * (foo_values + bar_values + no_text) + 4 * sample_df.with_missing.fillna(3)\n",
    "val += rng.normal(0, 8, size=SIZE)\n",
    "\n",
    "sample_df['label'] = np.where(val > np.median(val), 'a', 'b')\n",
    "\n",
    "print(sample_df.head())\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric']],\n",
    "                                                    pd.get_dummies(sample_df['label']), \n",
    "                                                    random_state=22)\n",
    "\n",
    "pl = Pipeline([('clf', OneVsRestClassifier(LogisticRegression()))])\n",
    "pl.fit(X_train, y_train)\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "\n",
    "print(\"\\nAccuracy on sample data - numeric, no nans: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to incorporate numeric data with missing values by adding a preprocessing step!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing numeric features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without imputing missing values, the pipeline would not be happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on sample data - numeric, inc nans:  0.636\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric', \"with_missing\"]],\n",
    "                                                    pd.get_dummies(sample_df['label']), \n",
    "                                                    random_state=456)\n",
    "pl = Pipeline([(\"imp\", SimpleImputer()),('clf', OneVsRestClassifier(LogisticRegression()))])\n",
    "pl.fit(X_train, y_train)\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "\n",
    "print(\"\\nAccuracy on sample data - numeric, inc nans: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text features and feature unions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the text column from the dataset we need to use countvectorizer but pipline steps for numeric and text preprocessing can't follow each other. Separately operate on the text and numeric columns is needed. FunctionTransformer and FeatureUnion can help to work with both text and numeric data. \n",
    "\n",
    "FunctionTransformer: turns a python function into an object that a scikit-learn pipeline can understand. They can be split into 2 different pipelines.\n",
    "\n",
    "FeatureUnion binds them. that'll be the input for the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on sample data - just text data:  0.808\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df[\"text\"],\n",
    "                                                    pd.get_dummies(sample_df['label']), \n",
    "                                                    random_state=456)\n",
    "pl = Pipeline([(\"vec\", CountVectorizer()),('clf', OneVsRestClassifier(LogisticRegression()))])\n",
    "pl.fit(X_train, y_train)\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on sample data - just text data: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple types of processing: FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Data\n",
      "0           \n",
      "1        foo\n",
      "2    foo bar\n",
      "3           \n",
      "4    foo bar\n",
      "Name: text, dtype: object\n",
      "\n",
      "Numeric Data\n",
      "     numeric  with_missing\n",
      "0 -10.856306      4.433240\n",
      "1   9.973454      4.310229\n",
      "2   2.829785      2.469828\n",
      "3 -15.062947      2.852981\n",
      "4  -5.786003      1.826475\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "get_text_data = FunctionTransformer(lambda x: x[\"text\"], validate=False)\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[[\"numeric\", \"with_missing\"]], validate=False)\n",
    "\n",
    "just_text_data = get_text_data.fit_transform(sample_df)\n",
    "just_numeric_data = get_numeric_data.fit_transform(sample_df)\n",
    "\n",
    "print(\"Text Data\")\n",
    "print(just_text_data.head())\n",
    "\n",
    "print(\"\\nNumeric Data\")\n",
    "print(just_numeric_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple types of processing: FeatureUnion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the results together using FeatureUnion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on sample data - all data:  0.928\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df[[\"numeric\", \"with_missing\", \"text\"]],\n",
    "                                                   pd.get_dummies(sample_df[\"label\"]), random_state=22)\n",
    "\n",
    "process_and_join_features = FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', SimpleImputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    (\"selector\", get_text_data),\n",
    "                    (\"vectorizer\", CountVectorizer())\n",
    "                ]))\n",
    "             ]\n",
    "        )\n",
    "\n",
    "# Instantiate nested pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', process_and_join_features),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "pl.fit(X_train, y_train)\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on sample data - all data: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To precess the text in the budget dataset, all of the text columns needed to be in the single column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using FunctionTransformer on the main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilabel_sample(y, size=1000, min_count=2, seed=None):\n",
    "    \"\"\" Takes a matrix of binary labels `y` and returns\n",
    "        the indices for a sample of size `size` if\n",
    "        `size` > 1 or `size` * len(y) if size =< 1.\n",
    "        The sample is guaranteed to have > `min_count` of\n",
    "        each label.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if (np.unique(y).astype(int) != np.array([0, 1])).any():\n",
    "            raise ValueError()\n",
    "    except (TypeError, ValueError):\n",
    "        raise ValueError('multilabel_sample only works with binary indicator matrices')\n",
    "\n",
    "    if (y.sum(axis=0) < min_count).any():\n",
    "        raise ValueError('Some classes do not have enough examples. Change min_count if necessary.')\n",
    "\n",
    "    if size <= 1:\n",
    "        size = np.floor(y.shape[0] * size)\n",
    "\n",
    "    if y.shape[1] * min_count > size:\n",
    "        msg = \"Size less than number of columns * min_count, returning {} items instead of {}.\"\n",
    "        warn(msg.format(y.shape[1] * min_count, size))\n",
    "        size = y.shape[1] * min_count\n",
    "\n",
    "    rng = np.random.RandomState(seed if seed is not None else np.random.randint(1))\n",
    "\n",
    "    if isinstance(y, pd.DataFrame):\n",
    "        choices = y.index\n",
    "        y = y.values\n",
    "    else:\n",
    "        choices = np.arange(y.shape[0])\n",
    "\n",
    "    sample_idxs = np.array([], dtype=choices.dtype)\n",
    "\n",
    "    # first, guarantee > min_count of each label\n",
    "    for j in range(y.shape[1]):\n",
    "        label_choices = choices[y[:, j] == 1]\n",
    "        label_idxs_sampled = rng.choice(label_choices, size=min_count, replace=False)\n",
    "        sample_idxs = np.concatenate([label_idxs_sampled, sample_idxs])\n",
    "\n",
    "    sample_idxs = np.unique(sample_idxs)\n",
    "\n",
    "    # now that we have at least min_count of each, we can just random sample\n",
    "    sample_count = int(size - sample_idxs.shape[0])\n",
    "\n",
    "    # get sample_count indices from remaining choices\n",
    "    remaining_choices = np.setdiff1d(choices, sample_idxs)\n",
    "    remaining_sampled = rng.choice(remaining_choices,\n",
    "                                   size=sample_count,\n",
    "                                   replace=False)\n",
    "\n",
    "    return np.concatenate([sample_idxs, remaining_sampled])\n",
    "\n",
    "\n",
    "def multilabel_sample_dataframe(df, labels, size, min_count=5, seed=None):\n",
    "    \"\"\" Takes a dataframe `df` and returns a sample of size `size` where all\n",
    "        classes in the binary matrix `labels` are represented at\n",
    "        least `min_count` times.\n",
    "    \"\"\"\n",
    "    idxs = multilabel_sample(labels, size=size, min_count=min_count, seed=seed)\n",
    "    return df.loc[idxs]\n",
    "\n",
    "\n",
    "def multilabel_train_test_split(X, Y, size, min_count=2, seed=None):\n",
    "    \"\"\" Takes a features matrix `X` and a label matrix `Y` and\n",
    "        returns (X_train, X_test, Y_train, Y_test) where all\n",
    "        classes in Y are represented at least `min_count` times.\n",
    "    \"\"\"\n",
    "    index = Y.index if isinstance(Y, pd.DataFrame) else np.arange(Y.shape[0])\n",
    "\n",
    "    test_set_idxs = multilabel_sample(Y, size=size, min_count=min_count, seed=seed)\n",
    "    train_set_idxs = np.setdiff1d(index, test_set_idxs)\n",
    "\n",
    "    test_set_mask = index.isin(test_set_idxs)\n",
    "    train_set_mask = ~test_set_mask\n",
    "\n",
    "    return (X[train_set_mask], X[test_set_mask], Y[train_set_mask], Y[test_set_mask])\n",
    "\n",
    "import pickle\n",
    "f = open(\"index.list\", \"rb\")\n",
    "index = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "df = pd.read_csv(\"TrainingData.csv\", index_col=0)\n",
    "df = df.loc[index]\n",
    "\n",
    "\n",
    "NUMERIC_COLUMNS = ['FTE', 'Total']\n",
    "LABELS = ['Function', 'Use', 'Sharing', 'Reporting', 'Student_Type', 'Position_Type', 'Object_Type', 'Pre_K', 'Operating_Status']\n",
    "NON_LABELS = [c for c in df.columns if c not in LABELS]\n",
    "def combine_text_columns(data_frame, to_drop=NUMERIC_COLUMNS + LABELS):\n",
    "    to_drop = set(to_drop) & set(data_frame.columns.tolist())\n",
    "    text_data = data_frame.drop(to_drop, axis=1)\n",
    "    text_data.fillna(\"\",inplace=True)\n",
    "    return text_data.apply(lambda x: \" \".join(x), axis=1)\n",
    "\n",
    "\n",
    "dummy_labels = pd.get_dummies(df[LABELS])\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(df[NON_LABELS],\n",
    "                                                               dummy_labels,\n",
    "                                                               0.2, \n",
    "                                                               seed=123)\n",
    "\n",
    "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a model to the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on budget dataset:  0.0\n"
     ]
    }
   ],
   "source": [
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    (\"selector\", get_numeric_data),\n",
    "                    (\"imputer\", SimpleImputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    (\"selector\", get_text_data),\n",
    "                    (\"vectorizer\", CountVectorizer())\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression(max_iter=1000), n_jobs=-1))\n",
    "    ])\n",
    "\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a different class of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on budget dataset:  0.32051282051282054\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', SimpleImputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer())\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        (\"clf\", RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "pl.fit(X_train, y_train)\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you adjust the model or parameters to improve accuracy?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on budget dataset:  0.3301282051282051\n"
     ]
    }
   ],
   "source": [
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', SimpleImputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer())\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('clf', RandomForestClassifier(n_estimators=15))\n",
    "    ])\n",
    "\n",
    "pl.fit(X_train, y_train)\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
