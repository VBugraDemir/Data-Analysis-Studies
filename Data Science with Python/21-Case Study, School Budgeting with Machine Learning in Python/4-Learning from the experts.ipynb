{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning from the experts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning from the expert: processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deciding what's a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00a', '12', '1st', '2nd', '4th', '5th', '70h', '8', 'a', 'aaps']\n"
     ]
    }
   ],
   "source": [
    "def multilabel_sample(y, size=1000, min_count=3, seed=None):\n",
    "    \"\"\" Takes a matrix of binary labels `y` and returns\n",
    "        the indices for a sample of size `size` if\n",
    "        `size` > 1 or `size` * len(y) if size =< 1.\n",
    "        The sample is guaranteed to have > `min_count` of\n",
    "        each label.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if (np.unique(y).astype(int) != np.array([0, 1])).any():\n",
    "            raise ValueError()\n",
    "    except (TypeError, ValueError):\n",
    "        raise ValueError('multilabel_sample only works with binary indicator matrices')\n",
    "\n",
    "    if (y.sum(axis=0) < min_count).any():\n",
    "        raise ValueError('Some classes do not have enough examples. Change min_count if necessary.')\n",
    "\n",
    "    if size <= 1:\n",
    "        size = np.floor(y.shape[0] * size)\n",
    "\n",
    "    if y.shape[1] * min_count > size:\n",
    "        msg = \"Size less than number of columns * min_count, returning {} items instead of {}.\"\n",
    "        warn(msg.format(y.shape[1] * min_count, size))\n",
    "        size = y.shape[1] * min_count\n",
    "\n",
    "    rng = np.random.RandomState(seed if seed is not None else np.random.randint(1))\n",
    "\n",
    "    if isinstance(y, pd.DataFrame):\n",
    "        choices = y.index\n",
    "        y = y.values\n",
    "    else:\n",
    "        choices = np.arange(y.shape[0])\n",
    "\n",
    "    sample_idxs = np.array([], dtype=choices.dtype)\n",
    "\n",
    "    # first, guarantee > min_count of each label\n",
    "    for j in range(y.shape[1]):\n",
    "        label_choices = choices[y[:, j] == 1]\n",
    "        label_idxs_sampled = rng.choice(label_choices, size=min_count, replace=False)\n",
    "        sample_idxs = np.concatenate([label_idxs_sampled, sample_idxs])\n",
    "\n",
    "    sample_idxs = np.unique(sample_idxs)\n",
    "\n",
    "    # now that we have at least min_count of each, we can just random sample\n",
    "    sample_count = int(size - sample_idxs.shape[0])\n",
    "\n",
    "    # get sample_count indices from remaining choices\n",
    "    remaining_choices = np.setdiff1d(choices, sample_idxs)\n",
    "    remaining_sampled = rng.choice(remaining_choices,\n",
    "                                   size=sample_count,\n",
    "                                   replace=False)\n",
    "\n",
    "    return np.concatenate([sample_idxs, remaining_sampled])\n",
    "\n",
    "\n",
    "def multilabel_sample_dataframe(df, labels, size, min_count=5, seed=None):\n",
    "    \"\"\" Takes a dataframe `df` and returns a sample of size `size` where all\n",
    "        classes in the binary matrix `labels` are represented at\n",
    "        least `min_count` times.\n",
    "    \"\"\"\n",
    "    idxs = multilabel_sample(labels, size=size, min_count=min_count, seed=seed)\n",
    "    return df.loc[idxs]\n",
    "\n",
    "\n",
    "def multilabel_train_test_split(X, Y, size, min_count=3, seed=None):\n",
    "    \"\"\" Takes a features matrix `X` and a label matrix `Y` and\n",
    "        returns (X_train, X_test, Y_train, Y_test) where all\n",
    "        classes in Y are represented at least `min_count` times.\n",
    "    \"\"\"\n",
    "    index = Y.index if isinstance(Y, pd.DataFrame) else np.arange(Y.shape[0])\n",
    "\n",
    "    test_set_idxs = multilabel_sample(Y, size=size, min_count=min_count, seed=seed)\n",
    "    train_set_idxs = np.setdiff1d(index, test_set_idxs)\n",
    "\n",
    "    test_set_mask = index.isin(test_set_idxs)\n",
    "    train_set_mask = ~test_set_mask\n",
    "\n",
    "    return (X[train_set_mask], X[test_set_mask], Y[train_set_mask], Y[test_set_mask])\n",
    "\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "f = open(\"index.list\", \"rb\")\n",
    "index = pickle.load(f)\n",
    "f.close()\n",
    "df = pd.read_csv(\"TrainingData.csv\", index_col=0)\n",
    "df = df.loc[index]\n",
    "NUMERIC_COLUMNS = ['FTE', 'Total']\n",
    "LABELS = ['Function', 'Use', 'Sharing', 'Reporting', 'Student_Type', 'Position_Type', 'Object_Type', 'Pre_K', 'Operating_Status']\n",
    "NON_LABELS = [c for c in df.columns if c not in LABELS]\n",
    "\n",
    "dummy_labels = pd.get_dummies(df[LABELS])\n",
    "\n",
    "\n",
    "def combine_text_columns(data_frame, to_drop=NUMERIC_COLUMNS + LABELS):\n",
    "    to_drop = set(to_drop) & set(data_frame.columns.tolist())\n",
    "    text_data = data_frame.drop(to_drop, axis=1)\n",
    "    text_data.fillna(\"\",inplace=True)\n",
    "    return text_data.apply(lambda x: \" \".join(x), axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(df[NON_LABELS],\n",
    "                                                               dummy_labels,\n",
    "                                                               0.2, \n",
    "                                                               seed=123)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text_vector = combine_text_columns(X_train)\n",
    "\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "text_features = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
    "text_features.fit(text_vector)\n",
    "print(text_features.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram range in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dim_red step uses a scikit-learn function called SelectKBest(), applying something called the chi-squared test to select the K \"best\" features. The scale step uses a scikit-learn function called MaxAbsScaler() in order to squash the relevant features into the interval -1 to 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "# Select 300 best features\n",
    "chi_k = 300\n",
    "\n",
    "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', SimpleImputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                                                   ngram_range=(1, 2))),\n",
    "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('scale', MaxAbsScaler()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model, trained with numeric data only, yields logloss score: 1.3604676692550002\n",
      "\n",
      "Accuracy on budget dataset:  0.22435897435897437\n"
     ]
    }
   ],
   "source": [
    "pl.fit(X_train, y_train)\n",
    "\n",
    "BOX_PLOTS_COLUMN_INDICES = [range(0, 37),\n",
    " range(37, 48),\n",
    " range(48, 51),\n",
    " range(51, 76),\n",
    " range(76, 79),\n",
    " range(79, 82),\n",
    " range(82, 87),\n",
    " range(87, 96),\n",
    " range(96, 104)]\n",
    "def _multi_multi_log_loss(predicted,\n",
    "                          actual,\n",
    "                          class_column_indices=BOX_PLOTS_COLUMN_INDICES,\n",
    "                          eps=1e-15):\n",
    "    \"\"\" Multi class version of Logarithmic Loss metric as implemented on\n",
    "    DrivenData.org\n",
    "    \"\"\"\n",
    "    class_scores = np.ones(len(class_column_indices), dtype=np.float64)\n",
    "    \n",
    "    # calculate log loss for each set of columns that belong to a class:\n",
    "    for k, this_class_indices in enumerate(class_column_indices):\n",
    "        # get just the columns for this class\n",
    "        preds_k = predicted[:, this_class_indices].astype(np.float64)\n",
    "        \n",
    "        # normalize so probabilities sum to one (unless sum is zero, then we clip)\n",
    "        preds_k /= np.clip(preds_k.sum(axis=1).reshape(-1, 1), eps, np.inf)\n",
    "\n",
    "        actual_k = actual[:, this_class_indices]\n",
    "\n",
    "        # shrink predictions so\n",
    "        y_hats = np.clip(preds_k, eps, 1 - eps)\n",
    "        sum_logs = np.sum(actual_k * np.log(y_hats))\n",
    "        class_scores[k] = (-1.0 / actual.shape[0]) * sum_logs\n",
    "        \n",
    "    return np.average(class_scores)\n",
    "def score_submission(pred_path=\"./\", holdout_path='LabelData.csv'):\n",
    "    # this happens on the backend to get the score\n",
    "    holdout_labels = pd.get_dummies(pd.read_csv(holdout_path, index_col=0).apply(lambda x: x.astype('category'), axis=0))\n",
    "\n",
    "    preds = pd.read_csv(pred_path, index_col=0)\n",
    "    \n",
    "    # make sure that format is correct\n",
    "    assert (preds.columns == holdout_labels.columns).all()\n",
    "    assert (preds.index == holdout_labels.index).all()\n",
    "\n",
    "    return _multi_multi_log_loss(preds.values, holdout_labels.values)\n",
    "\n",
    "act_holdout = pd.read_csv(\"act_HoldoutData.csv\", index_col=0)\n",
    "predictions = pl.predict_proba(act_holdout[NON_LABELS])\n",
    "# don't need to fillna's since we applied SimpleImputer in the pipeline.\n",
    "\n",
    "prediction_df = pd.DataFrame(columns=pd.get_dummies(df[LABELS]).columns, index=act_holdout.index, \n",
    "                             data = predictions)\n",
    "prediction_df.to_csv(\"predictions2.csv\")\n",
    "score = score_submission(pred_path =\"predictions2.csv\")\n",
    "print('Your model, trained with numeric data only, yields logloss score: {}'.format(score))\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning from the expert: a stats trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since CountVectorizer creates sparse matrix SparseInteractions is used because PolynomialFeatures does not support sparse matrices. It is used to describe when tokens appear together.\n",
    "\n",
    "$$ \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 (x_1 \\times x_2) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement interaction modeling in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class SparseInteractions(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, degree=2, feature_name_separator=\"_\"):\n",
    "        self.degree = degree\n",
    "        self.feature_name_separator = feature_name_separator\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if not sparse.isspmatrix_csc(X):\n",
    "            X = sparse.csc_matrix(X)\n",
    "\n",
    "        if hasattr(X, \"columns\"):\n",
    "            self.orig_col_names = X.columns\n",
    "        else:\n",
    "            self.orig_col_names = np.array([str(i) for i in range(X.shape[1])])\n",
    "\n",
    "        spi = self._create_sparse_interactions(X)\n",
    "        return spi\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names\n",
    "\n",
    "    def _create_sparse_interactions(self, X):\n",
    "        out_mat = []\n",
    "        self.feature_names = self.orig_col_names.tolist()\n",
    "\n",
    "        for sub_degree in range(2, self.degree + 1):\n",
    "            for col_ixs in combinations(range(X.shape[1]), sub_degree):\n",
    "                # add name for new column\n",
    "                name = self.feature_name_separator.join(self.orig_col_names[list(col_ixs)])\n",
    "                self.feature_names.append(name)\n",
    "\n",
    "                # get column multiplications value\n",
    "                out = X[:, col_ixs[0]]\n",
    "                for j in col_ixs[1:]:\n",
    "                    out = out.multiply(X[:, j])\n",
    "\n",
    "                out_mat.append(out)\n",
    "\n",
    "        return sparse.hstack([X] + out_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:68: FutureWarning: Pass k=300 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model, trained with numeric data only, yields logloss score: 1.3193666283220828\n",
      "\n",
      "Accuracy on budget dataset:  0.3525641025641026\n"
     ]
    }
   ],
   "source": [
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', SimpleImputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                                                   ngram_range=(1, 2))),  \n",
    "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        (\"int\", SparseInteractions(degree=2)),\n",
    "        ('scale', MaxAbsScaler()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression(max_iter=1000)))\n",
    "    ])\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "act_holdout = pd.read_csv(\"act_HoldoutData.csv\", index_col=0)\n",
    "predictions = pl.predict_proba(act_holdout[NON_LABELS])\n",
    "# don't need to fillna's since we applied SimpleImputer in the pipeline.\n",
    "\n",
    "\n",
    "prediction_df = pd.DataFrame(columns=pd.get_dummies(df[LABELS]).columns, index=act_holdout.index, \n",
    "                             data = predictions)\n",
    "prediction_df.to_csv(\"predictions3.csv\")\n",
    "score = score_submission(pred_path =\"predictions3.csv\")\n",
    "print('Your model, trained with numeric data only, yields logloss score: {}'.format(score))\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To clear up the confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs of the below data can be helpful to understand the what both CountVectorizer and SparseInteractions (or PolynomialFeatures) do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         c1\n",
      "0     This is a sentence is\n",
      "1  This is another sentence\n",
      "2    third document is here\n",
      "{'this': 13, 'is': 5, 'sentence': 9, 'this is': 14, 'is sentence': 8, 'sentence is': 10, 'another': 0, 'is another': 6, 'another sentence': 1, 'third': 11, 'document': 2, 'here': 4, 'third document': 12, 'document is': 3, 'is here': 7}\n",
      "['another', 'another sentence', 'document', 'document is', 'here', 'is', 'is another', 'is here', 'is sentence', 'sentence', 'sentence is', 'third', 'third document', 'this', 'this is']\n",
      "(3, 15)\n",
      "[[0 0 0 0 0 2 0 0 1 1 1 0 0 1 1]\n",
      " [1 1 0 0 0 1 1 0 0 1 0 0 0 1 1]\n",
      " [0 0 1 1 1 1 0 1 0 0 0 1 1 0 0]]\n",
      "   another  another sentence  document  document is  here  is  is another  \\\n",
      "0        0                 0         0            0     0   2           0   \n",
      "1        1                 1         0            0     0   1           1   \n",
      "2        0                 0         1            1     1   1           0   \n",
      "\n",
      "   is here  is sentence  sentence  sentence is  third  third document  this  \\\n",
      "0        0            1         1            1      0               0     1   \n",
      "1        0            0         1            0      0               0     1   \n",
      "2        1            0         0            0      1               1     0   \n",
      "\n",
      "   this is  \n",
      "0        1  \n",
      "1        1  \n",
      "2        0  \n",
      "   0  1  2  3  4  5  6  7  8  9  ...  10_11  10_12  10_13  10_14  11_12  \\\n",
      "0  0  0  0  0  0  2  0  0  1  1  ...      0      0      1      1      0   \n",
      "1  1  1  0  0  0  1  1  0  0  1  ...      0      0      0      0      0   \n",
      "2  0  0  1  1  1  1  0  1  0  0  ...      0      0      0      0      1   \n",
      "\n",
      "   11_13  11_14  12_13  12_14  13_14  \n",
      "0      0      0      0      0      1  \n",
      "1      0      0      0      0      1  \n",
      "2      0      0      0      0      0  \n",
      "\n",
      "[3 rows x 120 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "d = {\"c1\":[\"This is a sentence is\", \"This is another sentence\", \"third document is here\"]}\n",
    "df1 = pd.DataFrame(d)\n",
    "print(df1)\n",
    "ct = CountVectorizer(ngram_range=(1, 2))\n",
    "a = ct.fit(df1[\"c1\"])\n",
    "print(a.vocabulary_)\n",
    "print(a.get_feature_names())\n",
    "names = a.get_feature_names()\n",
    "a = ct.transform(df1[\"c1\"])\n",
    "print(a.shape)\n",
    "print(a.toarray())\n",
    "\n",
    "df2 = pd.DataFrame(a.toarray(), columns=names)\n",
    "print(df2)\n",
    "\n",
    "sp = SparseInteractions(degree=2)\n",
    "values = sp.fit_transform(a).toarray()\n",
    "names = sp.fit(a).get_feature_names()\n",
    "df3 = pd.DataFrame(values, columns=names)\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning from the expert: the winning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A balance adding new features with the computational cost of additional columns is needed. 3-grams, 4-grams will have an enormous increase in the size of the array. As the array grows in size the more computational power is needed to fit the model. Hashing trick is a way of limiting the size of matrix without losing too much model accuracy.\n",
    "\n",
    "A hash function takes an input and outputs a hash value. Output can be limited. Some columns will have multiple columns that map to them. -> Dimensionality reduction.\n",
    "\n",
    "Instead of using the CountVectorizer that creates the bag of words representation, HashingVectorizer can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the hashing trick in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.160128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.160128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.480384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.320256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.160128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0 -0.160128\n",
       "1  0.160128\n",
       "2 -0.480384\n",
       "3 -0.320256\n",
       "4  0.160128"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "text_data = combine_text_columns(X_train)\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "hashing_vec = HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
    "hashed_text = hashing_vec.fit_transform(text_data)\n",
    "hashed_df = pd.DataFrame(hashed_text.data)\n",
    "hashed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the winning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:68: FutureWarning: Pass k=300 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model, trained with numeric data only, yields logloss score: 1.3179052059738843\n"
     ]
    }
   ],
   "source": [
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', SimpleImputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    (\"vectorizer\", HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                                                     alternate_sign=False, norm=None, binary=False,\n",
    "                                                     ngram_range=(1, 2))),\n",
    "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('int', SparseInteractions(degree=2)),\n",
    "        ('scale', MaxAbsScaler()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "pl.fit(X_train, y_train)\n",
    "predictions = pl.predict_proba(act_holdout[NON_LABELS])\n",
    "# don't need to fillna's since we applied SimpleImputer in the pipeline.\n",
    "\n",
    "\n",
    "prediction_df = pd.DataFrame(columns=pd.get_dummies(df[LABELS]).columns, index=act_holdout.index, \n",
    "                             data = predictions)\n",
    "prediction_df.to_csv(\"predictions4.csv\")\n",
    "score = score_submission(pred_path =\"predictions4.csv\")\n",
    "print('Your model, trained with numeric data only, yields logloss score: {}'.format(score))\n",
    "# computing accuracy doesn't make sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the performance is about the same, but this is expected since the HashingVectorizer should work the same as the CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Function_Aides Compensation</th>\n",
       "      <th>Function_Career &amp; Academic Counseling</th>\n",
       "      <th>Function_Communications</th>\n",
       "      <th>Function_Curriculum Development</th>\n",
       "      <th>Function_Data Processing &amp; Information Services</th>\n",
       "      <th>Function_Development &amp; Fundraising</th>\n",
       "      <th>Function_Enrichment</th>\n",
       "      <th>Function_Extended Time &amp; Tutoring</th>\n",
       "      <th>Function_Facilities &amp; Maintenance</th>\n",
       "      <th>Function_Facilities Planning</th>\n",
       "      <th>...</th>\n",
       "      <th>Object_Type_Rent/Utilities</th>\n",
       "      <th>Object_Type_Substitute Compensation</th>\n",
       "      <th>Object_Type_Supplies/Materials</th>\n",
       "      <th>Object_Type_Travel &amp; Conferences</th>\n",
       "      <th>Pre_K_NO_LABEL</th>\n",
       "      <th>Pre_K_Non PreK</th>\n",
       "      <th>Pre_K_PreK</th>\n",
       "      <th>Operating_Status_Non-Operating</th>\n",
       "      <th>Operating_Status_Operating, Not PreK-12</th>\n",
       "      <th>Operating_Status_PreK-12 Operating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>0.010336</td>\n",
       "      <td>0.001350</td>\n",
       "      <td>0.000950</td>\n",
       "      <td>0.002223</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.001552</td>\n",
       "      <td>0.036873</td>\n",
       "      <td>0.002458</td>\n",
       "      <td>0.002052</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002558</td>\n",
       "      <td>0.011131</td>\n",
       "      <td>0.003388</td>\n",
       "      <td>0.003058</td>\n",
       "      <td>0.008551</td>\n",
       "      <td>0.991075</td>\n",
       "      <td>0.002052</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>0.002805</td>\n",
       "      <td>0.996208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>0.008051</td>\n",
       "      <td>0.007658</td>\n",
       "      <td>0.003251</td>\n",
       "      <td>0.024367</td>\n",
       "      <td>0.005281</td>\n",
       "      <td>0.011267</td>\n",
       "      <td>0.322427</td>\n",
       "      <td>0.012399</td>\n",
       "      <td>0.010652</td>\n",
       "      <td>0.001297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019754</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>0.070853</td>\n",
       "      <td>0.022528</td>\n",
       "      <td>0.855529</td>\n",
       "      <td>0.153876</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>0.049759</td>\n",
       "      <td>0.022046</td>\n",
       "      <td>0.900228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>0.002439</td>\n",
       "      <td>0.004282</td>\n",
       "      <td>0.002263</td>\n",
       "      <td>0.010340</td>\n",
       "      <td>0.006531</td>\n",
       "      <td>0.005756</td>\n",
       "      <td>0.012235</td>\n",
       "      <td>0.007307</td>\n",
       "      <td>0.014390</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012697</td>\n",
       "      <td>0.894072</td>\n",
       "      <td>0.011019</td>\n",
       "      <td>0.009794</td>\n",
       "      <td>0.986357</td>\n",
       "      <td>0.011677</td>\n",
       "      <td>0.005030</td>\n",
       "      <td>0.009602</td>\n",
       "      <td>0.004888</td>\n",
       "      <td>0.989973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786</th>\n",
       "      <td>0.004543</td>\n",
       "      <td>0.007863</td>\n",
       "      <td>0.003393</td>\n",
       "      <td>0.035710</td>\n",
       "      <td>0.005777</td>\n",
       "      <td>0.012877</td>\n",
       "      <td>0.099831</td>\n",
       "      <td>0.016628</td>\n",
       "      <td>0.015982</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040161</td>\n",
       "      <td>0.006948</td>\n",
       "      <td>0.066109</td>\n",
       "      <td>0.035574</td>\n",
       "      <td>0.891680</td>\n",
       "      <td>0.090272</td>\n",
       "      <td>0.007728</td>\n",
       "      <td>0.040823</td>\n",
       "      <td>0.011226</td>\n",
       "      <td>0.944377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2643</th>\n",
       "      <td>0.006274</td>\n",
       "      <td>0.005827</td>\n",
       "      <td>0.004016</td>\n",
       "      <td>0.070786</td>\n",
       "      <td>0.006112</td>\n",
       "      <td>0.010107</td>\n",
       "      <td>0.170501</td>\n",
       "      <td>0.007108</td>\n",
       "      <td>0.016798</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020751</td>\n",
       "      <td>0.004496</td>\n",
       "      <td>0.051330</td>\n",
       "      <td>0.024462</td>\n",
       "      <td>0.675536</td>\n",
       "      <td>0.317759</td>\n",
       "      <td>0.006514</td>\n",
       "      <td>0.018316</td>\n",
       "      <td>0.009561</td>\n",
       "      <td>0.975119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272864</th>\n",
       "      <td>0.010935</td>\n",
       "      <td>0.011039</td>\n",
       "      <td>0.002428</td>\n",
       "      <td>0.060812</td>\n",
       "      <td>0.003559</td>\n",
       "      <td>0.007837</td>\n",
       "      <td>0.076758</td>\n",
       "      <td>0.014596</td>\n",
       "      <td>0.016202</td>\n",
       "      <td>0.001319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022875</td>\n",
       "      <td>0.005079</td>\n",
       "      <td>0.040123</td>\n",
       "      <td>0.019194</td>\n",
       "      <td>0.951532</td>\n",
       "      <td>0.056505</td>\n",
       "      <td>0.004628</td>\n",
       "      <td>0.037189</td>\n",
       "      <td>0.023021</td>\n",
       "      <td>0.916348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448362</th>\n",
       "      <td>0.001055</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.004705</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.997210</td>\n",
       "      <td>0.002516</td>\n",
       "      <td>0.001005</td>\n",
       "      <td>0.997258</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>0.003557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181209</th>\n",
       "      <td>0.052098</td>\n",
       "      <td>0.005101</td>\n",
       "      <td>0.002655</td>\n",
       "      <td>0.029381</td>\n",
       "      <td>0.008218</td>\n",
       "      <td>0.004262</td>\n",
       "      <td>0.006723</td>\n",
       "      <td>0.004204</td>\n",
       "      <td>0.006792</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006913</td>\n",
       "      <td>0.002779</td>\n",
       "      <td>0.016820</td>\n",
       "      <td>0.006454</td>\n",
       "      <td>0.902857</td>\n",
       "      <td>0.061477</td>\n",
       "      <td>0.008899</td>\n",
       "      <td>0.026660</td>\n",
       "      <td>0.036588</td>\n",
       "      <td>0.897919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364039</th>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.012982</td>\n",
       "      <td>0.002915</td>\n",
       "      <td>0.021123</td>\n",
       "      <td>0.004712</td>\n",
       "      <td>0.009823</td>\n",
       "      <td>0.145637</td>\n",
       "      <td>0.017952</td>\n",
       "      <td>0.014522</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021685</td>\n",
       "      <td>0.005034</td>\n",
       "      <td>0.079683</td>\n",
       "      <td>0.029895</td>\n",
       "      <td>0.878265</td>\n",
       "      <td>0.113958</td>\n",
       "      <td>0.005429</td>\n",
       "      <td>0.031535</td>\n",
       "      <td>0.017366</td>\n",
       "      <td>0.943719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142060</th>\n",
       "      <td>0.009333</td>\n",
       "      <td>0.007919</td>\n",
       "      <td>0.003644</td>\n",
       "      <td>0.018362</td>\n",
       "      <td>0.010546</td>\n",
       "      <td>0.005555</td>\n",
       "      <td>0.017969</td>\n",
       "      <td>0.003526</td>\n",
       "      <td>0.008545</td>\n",
       "      <td>0.001524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006180</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.020566</td>\n",
       "      <td>0.011063</td>\n",
       "      <td>0.884637</td>\n",
       "      <td>0.087412</td>\n",
       "      <td>0.004290</td>\n",
       "      <td>0.014467</td>\n",
       "      <td>0.017088</td>\n",
       "      <td>0.964478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Function_Aides Compensation  Function_Career & Academic Counseling  \\\n",
       "237                        0.010336                               0.001350   \n",
       "466                        0.008051                               0.007658   \n",
       "784                        0.002439                               0.004282   \n",
       "1786                       0.004543                               0.007863   \n",
       "2643                       0.006274                               0.005827   \n",
       "...                             ...                                    ...   \n",
       "272864                     0.010935                               0.011039   \n",
       "448362                     0.001055                               0.000879   \n",
       "181209                     0.052098                               0.005101   \n",
       "364039                     0.006452                               0.012982   \n",
       "142060                     0.009333                               0.007919   \n",
       "\n",
       "        Function_Communications  Function_Curriculum Development  \\\n",
       "237                    0.000950                         0.002223   \n",
       "466                    0.003251                         0.024367   \n",
       "784                    0.002263                         0.010340   \n",
       "1786                   0.003393                         0.035710   \n",
       "2643                   0.004016                         0.070786   \n",
       "...                         ...                              ...   \n",
       "272864                 0.002428                         0.060812   \n",
       "448362                 0.000367                         0.000735   \n",
       "181209                 0.002655                         0.029381   \n",
       "364039                 0.002915                         0.021123   \n",
       "142060                 0.003644                         0.018362   \n",
       "\n",
       "        Function_Data Processing & Information Services  \\\n",
       "237                                            0.001012   \n",
       "466                                            0.005281   \n",
       "784                                            0.006531   \n",
       "1786                                           0.005777   \n",
       "2643                                           0.006112   \n",
       "...                                                 ...   \n",
       "272864                                         0.003559   \n",
       "448362                                         0.000593   \n",
       "181209                                         0.008218   \n",
       "364039                                         0.004712   \n",
       "142060                                         0.010546   \n",
       "\n",
       "        Function_Development & Fundraising  Function_Enrichment  \\\n",
       "237                               0.001552             0.036873   \n",
       "466                               0.011267             0.322427   \n",
       "784                               0.005756             0.012235   \n",
       "1786                              0.012877             0.099831   \n",
       "2643                              0.010107             0.170501   \n",
       "...                                    ...                  ...   \n",
       "272864                            0.007837             0.076758   \n",
       "448362                            0.004705             0.000884   \n",
       "181209                            0.004262             0.006723   \n",
       "364039                            0.009823             0.145637   \n",
       "142060                            0.005555             0.017969   \n",
       "\n",
       "        Function_Extended Time & Tutoring  Function_Facilities & Maintenance  \\\n",
       "237                              0.002458                           0.002052   \n",
       "466                              0.012399                           0.010652   \n",
       "784                              0.007307                           0.014390   \n",
       "1786                             0.016628                           0.015982   \n",
       "2643                             0.007108                           0.016798   \n",
       "...                                   ...                                ...   \n",
       "272864                           0.014596                           0.016202   \n",
       "448362                           0.001197                           0.000550   \n",
       "181209                           0.004204                           0.006792   \n",
       "364039                           0.017952                           0.014522   \n",
       "142060                           0.003526                           0.008545   \n",
       "\n",
       "        Function_Facilities Planning  ...  Object_Type_Rent/Utilities  \\\n",
       "237                         0.000632  ...                    0.002558   \n",
       "466                         0.001297  ...                    0.019754   \n",
       "784                         0.001135  ...                    0.012697   \n",
       "1786                        0.001394  ...                    0.040161   \n",
       "2643                        0.001416  ...                    0.020751   \n",
       "...                              ...  ...                         ...   \n",
       "272864                      0.001319  ...                    0.022875   \n",
       "448362                      0.000419  ...                    0.000591   \n",
       "181209                      0.001427  ...                    0.006913   \n",
       "364039                      0.001404  ...                    0.021685   \n",
       "142060                      0.001524  ...                    0.006180   \n",
       "\n",
       "        Object_Type_Substitute Compensation  Object_Type_Supplies/Materials  \\\n",
       "237                                0.011131                        0.003388   \n",
       "466                                0.004455                        0.070853   \n",
       "784                                0.894072                        0.011019   \n",
       "1786                               0.006948                        0.066109   \n",
       "2643                               0.004496                        0.051330   \n",
       "...                                     ...                             ...   \n",
       "272864                             0.005079                        0.040123   \n",
       "448362                             0.000723                        0.000280   \n",
       "181209                             0.002779                        0.016820   \n",
       "364039                             0.005034                        0.079683   \n",
       "142060                             0.002853                        0.020566   \n",
       "\n",
       "        Object_Type_Travel & Conferences  Pre_K_NO_LABEL  Pre_K_Non PreK  \\\n",
       "237                             0.003058        0.008551        0.991075   \n",
       "466                             0.022528        0.855529        0.153876   \n",
       "784                             0.009794        0.986357        0.011677   \n",
       "1786                            0.035574        0.891680        0.090272   \n",
       "2643                            0.024462        0.675536        0.317759   \n",
       "...                                  ...             ...             ...   \n",
       "272864                          0.019194        0.951532        0.056505   \n",
       "448362                          0.000788        0.997210        0.002516   \n",
       "181209                          0.006454        0.902857        0.061477   \n",
       "364039                          0.029895        0.878265        0.113958   \n",
       "142060                          0.011063        0.884637        0.087412   \n",
       "\n",
       "        Pre_K_PreK  Operating_Status_Non-Operating  \\\n",
       "237       0.002052                        0.002629   \n",
       "466       0.005274                        0.049759   \n",
       "784       0.005030                        0.009602   \n",
       "1786      0.007728                        0.040823   \n",
       "2643      0.006514                        0.018316   \n",
       "...            ...                             ...   \n",
       "272864    0.004628                        0.037189   \n",
       "448362    0.001005                        0.997258   \n",
       "181209    0.008899                        0.026660   \n",
       "364039    0.005429                        0.031535   \n",
       "142060    0.004290                        0.014467   \n",
       "\n",
       "        Operating_Status_Operating, Not PreK-12  \\\n",
       "237                                    0.002805   \n",
       "466                                    0.022046   \n",
       "784                                    0.004888   \n",
       "1786                                   0.011226   \n",
       "2643                                   0.009561   \n",
       "...                                         ...   \n",
       "272864                                 0.023021   \n",
       "448362                                 0.000630   \n",
       "181209                                 0.036588   \n",
       "364039                                 0.017366   \n",
       "142060                                 0.017088   \n",
       "\n",
       "        Operating_Status_PreK-12 Operating  \n",
       "237                               0.996208  \n",
       "466                               0.900228  \n",
       "784                               0.989973  \n",
       "1786                              0.944377  \n",
       "2643                              0.975119  \n",
       "...                                    ...  \n",
       "272864                            0.916348  \n",
       "448362                            0.003557  \n",
       "181209                            0.897919  \n",
       "364039                            0.943719  \n",
       "142060                            0.964478  \n",
       "\n",
       "[2000 rows x 104 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
